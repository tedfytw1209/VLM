{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiment\n",
    "\n",
    "## Setup environment\n",
    "\n",
    "Before running the experiments, run the following command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "make demo_m3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import base64\n",
    "import itertools\n",
    "import json\n",
    "import logging\n",
    "import time\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import tempfile\n",
    "from copy import deepcopy\n",
    "from io import BytesIO\n",
    "from pathlib import Path\n",
    "from shutil import copyfile, move, rmtree\n",
    "\n",
    "from uuid import uuid4\n",
    "from zipfile import ZipFile\n",
    "\n",
    "# Third-party imports\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "import requests\n",
    "import skimage\n",
    "import torch\n",
    "from monai.bundle import create_workflow\n",
    "from monai.transforms import (\n",
    "    Compose,\n",
    "    LoadImageD,\n",
    "    MapTransform,\n",
    "    OrientationD,\n",
    "    ScaleIntensityD,\n",
    "    ScaleIntensityRangeD,\n",
    ")\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Initialize logger\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.addHandler(logging.StreamHandler())\n",
    "\n",
    "# Constants\n",
    "SYS_PROMPT = None  # only useful in local mode\n",
    "REMOTE_URL = \"https://developer.download.nvidia.com/assets/Clara/monai/samples\"\n",
    "SEGMENTATION_TOKEN = \"<segmentation>\"\n",
    "\n",
    "MODEL_CARDS = (\n",
    "    \"Here is a list of available expert models:\\n\"\n",
    "    \"<BRATS(args)> \"\n",
    "        \"Modality: MRI, \"\n",
    "        \"Task: segmentation, \"\n",
    "        \"Overview: A pre-trained model for volumetric (3D) segmentation of brain tumor subregions from multimodal MRIs based on BraTS 2018 data, \"\n",
    "        \"Accuracy: Tumor core (TC): 0.8559 - Whole tumor (WT): 0.9026 - Enhancing tumor (ET): 0.7905 - Average: 0.8518, \"\n",
    "        \"Valid args are: None\\n\"\n",
    "    \"<VISTA3D(args)> \"\n",
    "        \"Modality: CT, \"\n",
    "        \"Task: segmentation, \"\n",
    "        \"Overview: domain-specialized interactive foundation model developed for segmenting and annotating human anatomies with precision, \"\n",
    "        \"Accuracy: 127 organs: 0.792 Dice on average, \"\n",
    "        \"Valid args are: 'everything', 'hepatic tumor', 'pancreatic tumor', 'lung tumor', 'bone lesion', 'organs', 'cardiovascular', 'gastrointestinal', 'skeleton', or 'muscles'\\n\"\n",
    "    \"<VISTA2D(args)> \"\n",
    "        \"Modality: cell imaging, \"\n",
    "        \"Task: segmentation, \"\n",
    "        \"Overview: model for cell segmentation, which was trained on a variety of cell imaging outputs, including brightfield, phase-contrast, fluorescence, confocal, or electron microscopy, \"\n",
    "        \"Accuracy: Good accuracy across several cell imaging datasets, \"\n",
    "        \"Valid args are: None\\n\"\n",
    "    \"<CXR(args)> \"\n",
    "        \"Modality: chest x-ray (CXR), \"\n",
    "        \"Task: classification, \"\n",
    "        \"Overview: pre-trained model which are trained on large cohorts of data, \"\n",
    "        \"Accuracy: Good accuracy across several diverse chest x-rays datasets, \"\n",
    "        \"Valid args are: None\\n\"\n",
    "    \"Give the model <NAME(args)> when selecting a suitable expert model.\\n\"\n",
    ")\n",
    "\n",
    "\n",
    "MODALITY_MAP = {\n",
    "    \"cxr\": \"CXR\",\n",
    "    \"chest x-ray\": \"CXR\",\n",
    "    \"ct image\": \"CT\",\n",
    "    \"mri\": \"MRI\",\n",
    "    \"magnetic resonance imaging\": \"MRI\",\n",
    "    \"ultrasound\": \"US\",\n",
    "    \"cell imaging\": \"cell imaging\",\n",
    "}\n",
    "\n",
    "\n",
    "class Dye(MapTransform):\n",
    "    \"\"\"\n",
    "    Dye the label map with predefined colors and write the image and label to disk.\n",
    "\n",
    "    Args:\n",
    "        slice_index: the index of the slice to be dyed. If None, the middle slice will be picked.\n",
    "        axis: the axis of the slice.\n",
    "        image_key: the key to extract the image data.\n",
    "        label_key: the key to extract the label data.\n",
    "        image_filename: the filename to save the image.\n",
    "        label_filename: the filename to save the label.\n",
    "        output_dir: the directory to save the image and label.\n",
    "        bg_label: the label value for the background.\n",
    "    \"\"\"\n",
    "\n",
    "    COLORS = [\n",
    "        \"red\",\n",
    "        \"blue\",\n",
    "        \"yellow\",\n",
    "        \"magenta\",\n",
    "        \"green\",\n",
    "        \"indigo\",\n",
    "        \"darkorange\",\n",
    "        \"cyan\",\n",
    "        \"pink\",\n",
    "        \"brown\",\n",
    "        \"orange\",\n",
    "        \"lime\",\n",
    "        \"orange\",\n",
    "        \"gold\",\n",
    "        \"yellowgreen\",\n",
    "        \"darkgreen\",\n",
    "    ]\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        slice_index: int | None = None,\n",
    "        axis: int = 2,\n",
    "        image_key: str = \"image\",\n",
    "        label_key: str = \"label\",\n",
    "        image_filename: str = \"image.jpg\",\n",
    "        label_filename: str = \"label.jpg\",\n",
    "        output_dir: Path = Path(\".\"),\n",
    "        bg_label: int = 0,\n",
    "    ):\n",
    "        \"\"\"Initialize the dye transform.\"\"\"\n",
    "        self.slice_index = slice_index\n",
    "        self.axis = axis\n",
    "        self.image_key = image_key\n",
    "        self.label_key = label_key\n",
    "        self.image_filename = image_filename\n",
    "        self.label_filename = label_filename\n",
    "        self.output_dir = Path(output_dir)\n",
    "        self.bg_label = bg_label\n",
    "        self.keys = [self.image_key, self.label_key]\n",
    "        self.allow_missing_keys = True\n",
    "\n",
    "    def __call__(self, data):\n",
    "        \"\"\"Dye the label map with predefined colors and write the image and label to disk.\"\"\"\n",
    "        d = dict(data)\n",
    "        for key in self.key_iterator(d):\n",
    "            np_array = np.squeeze(d.get(key))\n",
    "            slice_index = np_array.shape[2] // 2 if self.slice_index is None else self.slice_index\n",
    "            slice = np.take(np_array, slice_index, axis=self.axis)\n",
    "            d[key] = np.rot90(np.swapaxes(slice.astype(np.uint8), 0, 1), k=2)\n",
    "\n",
    "        os.makedirs(self.output_dir, exist_ok=True)\n",
    "        skimage.io.imsave(self.output_dir / self.image_filename, np.stack([d[self.image_key]] * 3, axis=-1))\n",
    "\n",
    "        if self.label_key in d:\n",
    "            color_label = (\n",
    "                skimage.color.label2rgb(\n",
    "                    d[self.label_key], colors=self.COLORS, image=d[self.image_key], bg_label=self.bg_label\n",
    "                )\n",
    "                * 255\n",
    "            )\n",
    "\n",
    "            skimage.io.imsave(self.output_dir / self.label_filename, color_label.astype(np.uint8))\n",
    "\n",
    "            unique_labels = np.unique(d[self.label_key])\n",
    "            color_cyle = itertools.cycle(Dye.COLORS)\n",
    "\n",
    "            colormap = {}\n",
    "            unique_labels = unique_labels[unique_labels != self.bg_label]  # remove background label\n",
    "            for label_id, label_color in zip(unique_labels, color_cyle):\n",
    "                colormap[label_id] = label_color\n",
    "            d[\"colormap\"] = colormap\n",
    "        return d\n",
    "\n",
    "\n",
    "def load_image(image_path_or_data_url: str) -> Image:\n",
    "    \"\"\"\n",
    "    Load the image from the URL.\n",
    "\n",
    "    Args:\n",
    "        image: the image URL or the base64 encoded image that starts with \"data:image\".\n",
    "\n",
    "    Returns:\n",
    "        PIL.Image: the loaded image.\n",
    "    \"\"\"\n",
    "    logger.debug(f\"Loading image from URL\")\n",
    "\n",
    "    if os.path.exists(image_path_or_data_url):\n",
    "        try:\n",
    "            return Image.open(image_path_or_data_url).convert(\"RGB\")\n",
    "        except Exception as e:\n",
    "            raise ValueError(f\"Failed to load the image: {e}\")\n",
    "    else:\n",
    "        image_base64_regex = re.compile(r\"^data:image/(png|jpe?g);base64,(.*)$\")\n",
    "        match_results = image_base64_regex.match(image_path_or_data_url)\n",
    "        if match_results:\n",
    "            image_base64 = match_results.groups()[1]\n",
    "            return Image.open(BytesIO(base64.b64decode(image_base64))).convert(\"RGB\")\n",
    "\n",
    "    raise ValueError(f\"Unable to load the image from {image_path_or_data_url[:50]}\")\n",
    "\n",
    "\n",
    "def save_image_url_to_file(image_url: str, output_dir: Path) -> str:\n",
    "    \"\"\"Save the image from the URL to the output directory\"\"\"\n",
    "    file_name = os.path.join(output_dir, image_url.split(\"/\")[-1])\n",
    "    # avoid re-downloading the image if it's already downloaded before\n",
    "    if not os.path.exists(file_name):\n",
    "        url_response = requests.get(image_url, allow_redirects=True)\n",
    "        with open(file_name, \"wb\") as f:\n",
    "            f.write(url_response.content)\n",
    "    return file_name\n",
    "\n",
    "\n",
    "def get_slice_filenames(image_file: str, slice_index: int, ext: str = \"jpg\"):\n",
    "    \"\"\"Small helper function to get the slice filenames\"\"\"\n",
    "    base_name = os.path.basename(image_file)\n",
    "    return base_name.replace(\".nii.gz\", f\"_slice{slice_index}_img.{ext}\")\n",
    "\n",
    "\n",
    "def _ct_chat_template(input_text):\n",
    "    \"\"\"Apply the chat template to the input text\"\"\"\n",
    "    return MODEL_CARDS + \"<image>\" + \"This is a CT image.\\n\" + input_text\n",
    "\n",
    "\n",
    "def apply_chat_template(input_text, with_image=True):\n",
    "    \"\"\"Apply the chat template to the input text\"\"\"\n",
    "    if with_image:\n",
    "        return _ct_chat_template(input_text)\n",
    "    else:\n",
    "        return input_text\n",
    "\n",
    "\n",
    "def get_monai_transforms(\n",
    "    keys,\n",
    "    output_dir: Path | str,\n",
    "    image_key=\"image\",\n",
    "    modality: str = \"CT\",\n",
    "    slice_index: int | None = None,\n",
    "    axis: int = 2,\n",
    "    image_filename: str = \"image.jpg\",\n",
    "    label_filename: str = \"label.jpg\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Get the MONAI transforms for the modality.\n",
    "\n",
    "    Args:\n",
    "        keys: the keys.\n",
    "        output_dir: the output directory.\n",
    "        image_key: the image key.\n",
    "        modality: the modality.\n",
    "        slice_index: the slice index.\n",
    "        axis: the axis.\n",
    "    \"\"\"\n",
    "    logger.debug(f\"Getting MONAI transforms for modality: {modality}\")\n",
    "    if image_key not in keys:\n",
    "        raise ValueError(f\"Image key {image_key} not found in the keys: {keys}\")\n",
    "\n",
    "    if modality == \"CT\":\n",
    "        # abdomen soft tissue https://radiopaedia.org/articles/windowing-ct\n",
    "        window_center = 50\n",
    "        window_width = 400\n",
    "        scaler = ScaleIntensityRangeD(\n",
    "            keys=[image_key],\n",
    "            a_min=window_center - window_width / 2,\n",
    "            a_max=window_center + window_width / 2,\n",
    "            b_min=0,\n",
    "            b_max=255,\n",
    "            clip=True,\n",
    "        )\n",
    "    elif modality == \"MRI\":\n",
    "        scaler = ScaleIntensityD(keys=[image_key], minv=0, maxv=255, channel_wise=True)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported modality: {modality}. Supported modalities are 'CT' and 'MRI'.\")\n",
    "\n",
    "    return Compose(\n",
    "        [\n",
    "            LoadImageD(keys=keys, ensure_channel_first=True),\n",
    "            OrientationD(keys=keys, axcodes=\"RAS\"),\n",
    "            scaler,\n",
    "            Dye(\n",
    "                slice_index=slice_index,\n",
    "                axis=axis,\n",
    "                output_dir=output_dir,\n",
    "                image_filename=image_filename,\n",
    "                label_filename=label_filename,\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "\n",
    "def image_to_data_url(image, format=\"JPEG\", max_size=None):\n",
    "    \"\"\"\n",
    "    Convert an image to a data URL.\n",
    "\n",
    "    Args:\n",
    "        image (str | np.Array): The image to convert. If a string, it is treated as a file path.\n",
    "        format (str): The format to save the image in. Default is \"JPEG\".\n",
    "        max_size (tuple): The maximum size of the image. Default is None.\n",
    "    \"\"\"\n",
    "    if isinstance(image, str) and os.path.exists(image):\n",
    "        img = Image.open(image)\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid image type: {type(image)}\")\n",
    "    if max_size is not None:\n",
    "        # Resize the image to the specified maximum height\n",
    "        img.thumbnail(max_size)\n",
    "    # Create a BytesIO buffer to save the image\n",
    "    buffered = BytesIO()\n",
    "    # Save the image to the buffer in the specified format\n",
    "    img.save(buffered, format=format)\n",
    "    # Convert the buffer content into bytes\n",
    "    img_byte = buffered.getvalue()\n",
    "    # Encode the bytes to base64\n",
    "    img_base64 = base64.b64encode(img_byte).decode()\n",
    "    # Convert the base64 bytes to string and format the data URL\n",
    "    return f\"data:image/{format.lower()};base64,{img_base64}\"\n",
    "\n",
    "\n",
    "def _get_modality_url(image_url_or_path: str | None):\n",
    "    \"\"\"\n",
    "    Extract image modality by checking the URL or file path.\n",
    "    If the URL or file path contains \".nii.gz\" and contain \"mri_\", then it is MRI, else it is CT.\n",
    "    If it contains \"cxr_\" then it is CXR, otherwise it is Unknown.\n",
    "    \"\"\"\n",
    "    if isinstance(image_url_or_path, list) and len(image_url_or_path) > 0:\n",
    "        image_url_or_path = image_url_or_path[0]\n",
    "    if not isinstance(image_url_or_path, str):\n",
    "        return \"Unknown\"\n",
    "    if image_url_or_path.startswith(\"data:image\"):\n",
    "        return \"Unknown\"\n",
    "    if \".nii.gz\" in image_url_or_path.lower():\n",
    "        if \"mri_\" in image_url_or_path.lower():\n",
    "            return \"MRI\"\n",
    "        return \"CT\"\n",
    "    if \"cxr_\" in image_url_or_path.lower():\n",
    "        return \"CXR\"\n",
    "    return \"Unknown\"\n",
    "\n",
    "\n",
    "def _get_modality_text(text: str):\n",
    "    \"\"\"Get the modality from the text\"\"\"\n",
    "    \n",
    "    if not text:\n",
    "        return \"Unknown\"\n",
    "    for keyword, modality in MODALITY_MAP.items():\n",
    "        if keyword.lower() in text.lower():\n",
    "            return modality\n",
    "    return \"Unknown\"\n",
    "\n",
    "\n",
    "def get_modality(image_url: str | None, text: str | None = None):\n",
    "    \"\"\"Get the modality from the image URL or text\"\"\"\n",
    "    logger.debug(f\"Getting modality from image URL or text\")\n",
    "    modality = _get_modality_url(image_url)\n",
    "    if modality != \"Unknown\":\n",
    "        return modality\n",
    "    return _get_modality_text(text)\n",
    "\n",
    "\n",
    "class ChatHistory:\n",
    "    \"\"\"Class to store the chat history\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Messages are stored as a list, with a sample format:\n",
    "\n",
    "        messages = [\n",
    "        # --------------- Below is the previous prompt from the user ---------------\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": \"What is in the image? <image>\"\n",
    "                },\n",
    "                {\n",
    "                    \"type\": \"image_path\",\n",
    "                    \"image_path\": image_path\n",
    "                }\n",
    "            ]\n",
    "        },\n",
    "        # --------------- Below is the answer from the previous completion ---------------\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": answer1,\n",
    "                }\n",
    "            ]\n",
    "        },\n",
    "        ]\n",
    "        \"\"\"\n",
    "        self.messages = []\n",
    "        self.last_prompt_with_image = None\n",
    "\n",
    "    def append(self, prompt_or_answer, image_path=None, role=\"user\"):\n",
    "        \"\"\"\n",
    "        Append a new message to the chat history.\n",
    "\n",
    "        Args:\n",
    "            prompt_or_answer (str): The text prompt from human or answer from AI to append.\n",
    "            image_url (str): The image file path to append.\n",
    "            slice_index (int): The slice index for 3D images.\n",
    "            role (str): The role of the message. Default is \"user\". Other option is \"assistant\" and \"expert\".\n",
    "        \"\"\"\n",
    "        new_contents = [\n",
    "            {\n",
    "                \"type\": \"text\",\n",
    "                \"text\": prompt_or_answer,\n",
    "            }\n",
    "        ]\n",
    "        if image_path is not None:\n",
    "            new_contents.append(\n",
    "                {\n",
    "                    \"type\": \"image_path\",\n",
    "                    \"image_path\": image_path,\n",
    "                }\n",
    "            )\n",
    "            self.last_prompt_with_image = prompt_or_answer\n",
    "\n",
    "        self.messages.append({\"role\": role, \"content\": new_contents})\n",
    "\n",
    "\n",
    "class ImageCache:\n",
    "    \"\"\"A simple image cache to store images and data URLs.\"\"\"\n",
    "\n",
    "    def __init__(self, cache_dir: Path):\n",
    "        \"\"\"Initialize the image cache.\"\"\"\n",
    "        cache_dir = Path(cache_dir)\n",
    "        if not cache_dir.exists():\n",
    "            cache_dir.mkdir(parents=True)\n",
    "        self.cache_dir = cache_dir\n",
    "        self.cache_images = {}\n",
    "\n",
    "    def cache(self, image_urls_or_paths):\n",
    "        \"\"\"Cache the images from the URLs or paths.\"\"\"\n",
    "        logger.debug(f\"Caching the image to {self.cache_dir}\")\n",
    "        for _, items in image_urls_or_paths.items():\n",
    "            items = items if isinstance(items, list) else [items]\n",
    "            for item in items:\n",
    "                if item.startswith(\"http\"):\n",
    "                    self.cache_images[item] = save_image_url_to_file(item, self.cache_dir)\n",
    "                elif os.path.exists(item):\n",
    "                    # move the file to the cache directory\n",
    "                    file_name = os.path.basename(item)\n",
    "                    self.cache_images[item] = os.path.join(self.cache_dir, file_name)\n",
    "                    if not os.path.isfile(self.cache_images[item]):\n",
    "                        copyfile(item, self.cache_images[item])\n",
    "\n",
    "                if self.cache_images[item].endswith(\".nii.gz\"):\n",
    "                    data = nib.load(self.cache_images[item]).get_fdata()\n",
    "                    for slice_index in tqdm(range(data.shape[2])):\n",
    "                        image_filename = get_slice_filenames(self.cache_images[item], slice_index)\n",
    "                        if not os.path.exists(os.path.join(self.cache_dir, image_filename)):\n",
    "                            compose = get_monai_transforms(\n",
    "                                [\"image\"],\n",
    "                                self.cache_dir,\n",
    "                                modality=get_modality(item),\n",
    "                                slice_index=slice_index,\n",
    "                                image_filename=image_filename,\n",
    "                            )\n",
    "                            compose({\"image\": self.cache_images[item]})\n",
    "\n",
    "    def cleanup(self):\n",
    "        \"\"\"Clean up the cache directory.\"\"\"\n",
    "        logger.debug(f\"Cleaning up the cache\")\n",
    "        rmtree(self.cache_dir)\n",
    "\n",
    "    def dir(self):\n",
    "        \"\"\"Return the cache directory.\"\"\"\n",
    "        return str(self.cache_dir)\n",
    "\n",
    "    def get(self, key: str | list, default=None, list_return=False):\n",
    "        \"\"\"Get the image or data URL from the cache.\"\"\"\n",
    "        if isinstance(key, list):\n",
    "            items = [self.cache_images.get(k) for k in key]\n",
    "            return items if list_return else items[0]\n",
    "        return self.cache_images.get(key, default)\n",
    "\n",
    "\n",
    "class SessionVariables:\n",
    "    \"\"\"Class to store the session variables\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize the session variables\"\"\"\n",
    "        global SYS_PROMPT\n",
    "        self.sys_prompt = SYS_PROMPT\n",
    "        self.sys_msg = MODEL_CARDS\n",
    "        self.use_model_cards = True\n",
    "        self.slice_index = None  # Slice index for 3D images\n",
    "        self.image_url = None  # Image URL to the image on the web\n",
    "        self.backup = {}  # Cached varaiables from previous messages for the current conversation\n",
    "        self.axis = 2\n",
    "        self.top_p = 0.9\n",
    "        self.temperature = 0.0\n",
    "        self.max_tokens = 1024\n",
    "        self.temp_working_dir = None\n",
    "        # self.idx_range = (None, None)\n",
    "        # self.interactive = False\n",
    "        self.sys_msgs_to_hide = []\n",
    "        self.modality_prompt = \"Auto\"\n",
    "\n",
    "    def restore_from_backup(self, attr):\n",
    "        \"\"\"Retrieve the attribute from the backup\"\"\"\n",
    "        attr_val = self.backup.get(attr, None)\n",
    "        if attr_val is not None:\n",
    "            self.__setattr__(attr, attr_val)\n",
    "\n",
    "\n",
    "class ExpertVista3D():\n",
    "    \"\"\"Expert model for VISTA-3D.\"\"\"\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        \"\"\"Initialize the VISTA-3D expert model.\"\"\"\n",
    "        self.model_name = \"VISTA3D\"\n",
    "        self.bundle_root = os.path.expanduser(\"~/.cache/torch/hub/bundle/vista3d_v0.5.4/vista3d\")\n",
    "\n",
    "    def _get_label_groups(self):\n",
    "        \"\"\"Get the label groups from the label groups path.\"\"\"\n",
    "        return {\n",
    "            \"everything\": \"../experts/vista3d/label_dict.json\",\n",
    "            \"hepatic tumor\": {\n",
    "                \"liver\": 1,\n",
    "                \"hepatic tumor\": 26\n",
    "            },\n",
    "            \"hepatoma\": {\n",
    "                \"liver\": 1,\n",
    "                \"hepatic tumor\": 26\n",
    "            },\n",
    "            \"pancreatic tumor\": {\n",
    "                \"pancreas\": 4,\n",
    "                \"pancreatic tumor\": 24\n",
    "            },\n",
    "            \"lung tumor\": {\n",
    "                \"lung\": 20,\n",
    "                \"lung tumor\": 23,\n",
    "                \"left lung upper lobe\": 28,\n",
    "                \"left lung lower lobe\": 29,\n",
    "                \"right lung upper lobe\": 30,\n",
    "                \"right lung middle lobe\": 31,\n",
    "                \"right lung lower lobe\": 32\n",
    "            },\n",
    "            \"bone lesion\": {\n",
    "                \"bone lesion\": 128\n",
    "            },\n",
    "            \"organs\": {\n",
    "                \"liver\": 1,\n",
    "                \"kidney\": 2,\n",
    "                \"spleen\": 3,\n",
    "                \"pancreas\": 4,\n",
    "                \"right kidney\": 5,\n",
    "                \"right adrenal gland\": 8,\n",
    "                \"left adrenal gland\": 9,\n",
    "                \"gallbladder\": 10,\n",
    "                \"left kidney\": 14,\n",
    "                \"brain\": 22,\n",
    "                \"lung tumor\": 23,\n",
    "                \"pancreatic tumor\": 24,\n",
    "                \"hepatic vessel\": 25,\n",
    "                \"hepatic tumor\": 26,\n",
    "                \"colon cancer primaries\": 27,\n",
    "                \"left lung upper lobe\": 28,\n",
    "                \"left lung lower lobe\": 29,\n",
    "                \"right lung upper lobe\": 30,\n",
    "                \"right lung middle lobe\": 31,\n",
    "                \"right lung lower lobe\": 32,\n",
    "                \"trachea\": 57,\n",
    "                \"left kidney cyst\": 116,\n",
    "                \"right kidney cyst\": 117,\n",
    "                \"prostate\": 118,\n",
    "                \"spinal cord\": 121,\n",
    "                \"thyroid gland\": 126,\n",
    "                \"airway\": 132\n",
    "            },\n",
    "            \"cardiovascular\": {\n",
    "                \"aorta\": 6,\n",
    "                \"inferior vena cava\": 7,\n",
    "                \"portal vein and splenic vein\": 17,\n",
    "                \"left iliac artery\": 58,\n",
    "                \"right iliac artery\": 59,\n",
    "                \"left iliac vena\": 60,\n",
    "                \"right iliac vena\": 61,\n",
    "                \"left atrial appendage\": 108,\n",
    "                \"brachiocephalic trunk\": 109,\n",
    "                \"left brachiocephalic vein\": 110,\n",
    "                \"right brachiocephalic vein\": 111,\n",
    "                \"left common carotid artery\": 112,\n",
    "                \"right common carotid artery\": 113,\n",
    "                \"heart\": 115,\n",
    "                \"pulmonary vein\": 119,\n",
    "                \"left subclavian artery\": 123,\n",
    "                \"right subclavian artery\": 124,\n",
    "                \"superior vena cava\": 125\n",
    "            },\n",
    "            \"gastrointestinal\": {\n",
    "                \"esophagus\": 11,\n",
    "                \"stomach\": 12,\n",
    "                \"duodenum\": 13,\n",
    "                \"bladder\": 15,\n",
    "                \"small bowel\": 19,\n",
    "                \"colon\": 62\n",
    "            },\n",
    "            \"skeleton\": {\n",
    "                \"bone\": 21,\n",
    "                \"vertebrae L5\": 33,\n",
    "                \"vertebrae L4\": 34,\n",
    "                \"vertebrae L3\": 35,\n",
    "                \"vertebrae L2\": 36,\n",
    "                \"vertebrae L1\": 37,\n",
    "                \"vertebrae T12\": 38,\n",
    "                \"vertebrae T11\": 39,\n",
    "                \"vertebrae T10\": 40,\n",
    "                \"vertebrae T9\": 41,\n",
    "                \"vertebrae T8\": 42,\n",
    "                \"vertebrae T7\": 43,\n",
    "                \"vertebrae T6\": 44,\n",
    "                \"vertebrae T5\": 45,\n",
    "                \"vertebrae T4\": 46,\n",
    "                \"vertebrae T3\": 47,\n",
    "                \"vertebrae T2\": 48,\n",
    "                \"vertebrae T1\": 49,\n",
    "                \"vertebrae C7\": 50,\n",
    "                \"vertebrae C6\": 51,\n",
    "                \"vertebrae C5\": 52,\n",
    "                \"vertebrae C4\": 53,\n",
    "                \"vertebrae C3\": 54,\n",
    "                \"vertebrae C2\": 55,\n",
    "                \"vertebrae C1\": 56,\n",
    "                \"skull\": 120,\n",
    "                \"sternum\": 122,\n",
    "                \"vertebrae S1\": 127,\n",
    "                \"bone lesion\": 128,\n",
    "                \"left rib 1\": 63,\n",
    "                \"left rib 2\": 64,\n",
    "                \"left rib 3\": 65,\n",
    "                \"left rib 4\": 66,\n",
    "                \"left rib 5\": 67,\n",
    "                \"left rib 6\": 68,\n",
    "                \"left rib 7\": 69,\n",
    "                \"left rib 8\": 70,\n",
    "                \"left rib 9\": 71,\n",
    "                \"left rib 10\": 72,\n",
    "                \"left rib 11\": 73,\n",
    "                \"left rib 12\": 74,\n",
    "                \"right rib 1\": 75,\n",
    "                \"right rib 2\": 76,\n",
    "                \"right rib 3\": 77,\n",
    "                \"right rib 4\": 78,\n",
    "                \"right rib 5\": 79,\n",
    "                \"right rib 6\": 80,\n",
    "                \"right rib 7\": 81,\n",
    "                \"right rib 8\": 82,\n",
    "                \"right rib 9\": 83,\n",
    "                \"right rib 10\": 84,\n",
    "                \"right rib 11\": 85,\n",
    "                \"right rib 12\": 86,\n",
    "                \"left humerus\": 87,\n",
    "                \"right humerus\": 88,\n",
    "                \"left scapula\": 89,\n",
    "                \"right scapula\": 90,\n",
    "                \"left clavicula\": 91,\n",
    "                \"right clavicula\": 92,\n",
    "                \"left femur\": 93,\n",
    "                \"right femur\": 94,\n",
    "                \"left hip\": 95,\n",
    "                \"right hip\": 96,\n",
    "                \"sacrum\": 97,\n",
    "                \"costal cartilages\": 114\n",
    "            },\n",
    "            \"muscles\": {\n",
    "                \"left gluteus maximus\": 98,\n",
    "                \"right gluteus maximus\": 99,\n",
    "                \"left gluteus medius\": 100,\n",
    "                \"right gluteus medius\": 101,\n",
    "                \"left gluteus minimus\": 102,\n",
    "                \"right gluteus minimus\": 103,\n",
    "                \"left autochthon\": 104,\n",
    "                \"right autochthon\": 105,\n",
    "                \"left iliopsoas\": 106,\n",
    "                \"right iliopsoas\": 107\n",
    "            }\n",
    "        }\n",
    "\n",
    "    def label_id_to_name(self, label_id: int, label_dict: dict):\n",
    "        \"\"\"\n",
    "        Get the label name from the label ID.\n",
    "\n",
    "        Args:\n",
    "            label_id: the label ID.\n",
    "            label_dict: the label dictionary.\n",
    "        \"\"\"\n",
    "        for group_dict in list(label_dict.values()):\n",
    "            if isinstance(group_dict, dict):\n",
    "                # this will skip str type value, such as \"everything\": <path>\n",
    "                for label_name, label_id_ in group_dict.items():\n",
    "                    if label_id == label_id_:\n",
    "                        return label_name\n",
    "        return None\n",
    "\n",
    "    def segmentation_to_string(\n",
    "        self,\n",
    "        output_dir: Path,\n",
    "        img_file: str,\n",
    "        seg_file: str,\n",
    "        label_groups: dict,\n",
    "        modality: str = \"CT\",\n",
    "        slice_index: int | None = None,\n",
    "        axis: int = 2,\n",
    "        image_filename: str = \"image.jpg\",\n",
    "        label_filename: str = \"label.jpg\",\n",
    "        output_prefix=None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Format the segmentation response to a string.\n",
    "\n",
    "        Args:\n",
    "            response: the response.\n",
    "            output_dir: the output directory.\n",
    "            img_file: the image file path.\n",
    "            modality: the modality.\n",
    "            slice_index: the slice index.\n",
    "            axis: the axis.\n",
    "            image_filename: the image filename for the sliced image.\n",
    "            label_filename: the label filename for the sliced image.\n",
    "            group_label_names: the group label names to filter the label names.\n",
    "            output_prefix: the output prefix.\n",
    "            label_groups_path: the label groups path for VISTA-3D.\n",
    "        \"\"\"\n",
    "        global SEGMENTATION_TOKEN\n",
    "        output_dir = Path(output_dir)\n",
    "        if output_prefix is None:\n",
    "            output_prefix = f\"The results are {SEGMENTATION_TOKEN}. The colors in this image describe \"\n",
    "\n",
    "        transforms = get_monai_transforms(\n",
    "            [\"image\", \"label\"],\n",
    "            output_dir,\n",
    "            modality=modality,\n",
    "            slice_index=slice_index,\n",
    "            axis=axis,\n",
    "            image_filename=image_filename,\n",
    "            label_filename=label_filename,\n",
    "        )\n",
    "        data = transforms({\"image\": img_file, \"label\": seg_file})\n",
    "\n",
    "        formatted_items = []\n",
    "\n",
    "        for label_id in data[\"colormap\"]:\n",
    "            label_name = self.label_id_to_name(label_id, label_groups)\n",
    "            if label_name is not None:\n",
    "                color = data[\"colormap\"][label_id]\n",
    "                formatted_items.append(f\"{color}: {label_name}\")\n",
    "\n",
    "        return output_prefix + \", \".join(formatted_items) + \". \"\n",
    "\n",
    "    def mentioned_by(self, input: str):\n",
    "        \"\"\"\n",
    "        Check if the VISTA-3D model is mentioned in the input.\n",
    "\n",
    "        Args:\n",
    "            input (str): Text from the LLM, e.g. \"Let me trigger <VISTA3D(arg)>.\"\n",
    "\n",
    "        Returns:\n",
    "            bool: True if the VISTA-3D model is mentioned, False otherwise.\n",
    "        \"\"\"\n",
    "        matches = re.findall(r\"<(.*?)>\", str(input))\n",
    "        if len(matches) != 1:\n",
    "            return False\n",
    "        return self.model_name in str(matches[0])\n",
    "\n",
    "    def download_file(self, url: str, img_file: str):\n",
    "        \"\"\"\n",
    "        Download the file from the URL.\n",
    "\n",
    "        Args:\n",
    "            url (str): The URL.\n",
    "            img_file (str): The file path.\n",
    "        \"\"\"\n",
    "        parent_dir = os.path.dirname(img_file)\n",
    "        os.makedirs(parent_dir, exist_ok=True)\n",
    "        with open(img_file, \"wb\") as f:\n",
    "            response = requests.get(url)\n",
    "            f.write(response.content)\n",
    "\n",
    "    def run(\n",
    "        self,\n",
    "        img_file: str = \"\",\n",
    "        image_url: str = \"\",\n",
    "        input: str = \"\",\n",
    "        output_dir: str = \"\",\n",
    "        slice_index: int = 0,\n",
    "        prompt: str = \"\",\n",
    "        **kwargs,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Run the VISTA-3D model.\n",
    "\n",
    "        Args:\n",
    "            image_url (str): The image URL.\n",
    "            input (str): The input text.\n",
    "            output_dir (str): The output directory.\n",
    "            img_file (str): The image file path. If not provided, download from the URL.\n",
    "            slice_index (int): The slice index.\n",
    "            prompt (str): The prompt text from the original request.\n",
    "            **kwargs: Additional keyword arguments.\n",
    "        \"\"\"\n",
    "        if not img_file:\n",
    "            # Download from the URL\n",
    "            img_file = os.path.join(output_dir, os.path.basename(image_url))\n",
    "            self.download_file(image_url, img_file)\n",
    "\n",
    "        output_dir = Path(output_dir)\n",
    "        matches = re.findall(r\"<(.*?)>\", input)\n",
    "        if len(matches) != 1:\n",
    "            raise ValueError(f\"Expert model {self.model_name} is not correctly enclosed in angle brackets.\")\n",
    "\n",
    "        match = matches[0]\n",
    "\n",
    "        # Extract the arguments\n",
    "        arg_matches = re.findall(r\"\\((.*?)\\)\", match[len(self.model_name) :])\n",
    "\n",
    "        if len(arg_matches) == 0:  # <VISTA3D>\n",
    "            arg_matches = [\"everything\"]\n",
    "        if len(arg_matches) == 1 and (arg_matches[0] == \"\" or arg_matches[0] == None):  # <VISTA3D()>\n",
    "            arg_matches = [\"everything\"]\n",
    "        if len(arg_matches) > 1:\n",
    "            raise ValueError(\n",
    "                \"Multiple expert model arguments are provided in the same prompt, \"\n",
    "                \"which is not supported in this version.\"\n",
    "            )\n",
    "\n",
    "        vista3d_prompts = None\n",
    "        label_groups = self._get_label_groups()\n",
    "\n",
    "        if arg_matches[0] not in label_groups:\n",
    "            raise ValueError(f\"Label group {arg_matches[0]} is not accepted by the VISTA-3D model.\")\n",
    "\n",
    "        if arg_matches[0] != \"everything\":\n",
    "            vista3d_prompts = [cls_idx for _, cls_idx in label_groups[arg_matches[0]].items()]\n",
    "\n",
    "        # Trigger the VISTA-3D model\n",
    "        input_dict = {\"image\": img_file}\n",
    "        if vista3d_prompts is not None:\n",
    "            input_dict[\"label_prompt\"] = vista3d_prompts\n",
    "\n",
    "        sys.path = [self.bundle_root] + sys.path\n",
    "\n",
    "        with tempfile.TemporaryDirectory() as temp_dir:\n",
    "            workflow = create_workflow(\n",
    "                workflow_type=\"infer\",\n",
    "                bundle_root=self.bundle_root,\n",
    "                config_file=os.path.join(self.bundle_root, f\"configs/inference.json\"),\n",
    "                logging_file=os.path.join(self.bundle_root, \"configs/logging.conf\"),\n",
    "                meta_file=os.path.join(self.bundle_root, \"configs/metadata.json\"),\n",
    "                input_dict=input_dict,\n",
    "                output_dtype=\"uint8\",\n",
    "                separate_folder=False,\n",
    "                output_ext=\".nii.gz\",\n",
    "                output_dir=temp_dir,\n",
    "            )\n",
    "            workflow.evaluator.run()\n",
    "            output_file = os.path.join(temp_dir, os.listdir(temp_dir)[0])\n",
    "            seg_file = os.path.join(output_dir, \"segmentation.nii.gz\")\n",
    "            move(output_file, seg_file)\n",
    "\n",
    "        seg_image = f\"seg_{uuid4()}.jpg\"\n",
    "        text_output = self.segmentation_to_string(\n",
    "            output_dir,\n",
    "            img_file,\n",
    "            seg_file,\n",
    "            label_groups,\n",
    "            modality=\"CT\",\n",
    "            slice_index=slice_index,\n",
    "            image_filename=get_slice_filenames(img_file, slice_index),\n",
    "            label_filename=seg_image,\n",
    "        )\n",
    "\n",
    "        if \"segmented\" in input:\n",
    "            instruction = \"\"  # no need to ask for instruction\n",
    "        else:\n",
    "            instruction = \"Use this result to respond to this prompt:\\n\" + prompt\n",
    "        return text_output, os.path.join(output_dir, seg_image), instruction\n",
    "\n",
    "\n",
    "class M3Generator:\n",
    "    \"\"\"Class to generate M3 responses\"\"\"\n",
    "\n",
    "    def __init__(self, cache_images, source=\"huggingface\", model_path=\"\", conv_mode=\"\", api_key=\"\"):\n",
    "        \"\"\"Initialize the M3 generator\"\"\"\n",
    "        global SYS_PROMPT\n",
    "        self.cache_images = cache_images\n",
    "        self.source = source\n",
    "        if source == \"local\" or source == \"huggingface\":\n",
    "            from llava.conversation import conv_templates\n",
    "            from llava.mm_utils import get_model_name_from_path\n",
    "            from llava.model.builder import load_pretrained_model\n",
    "            from llava.utils import disable_torch_init\n",
    "\n",
    "            # Here we rewrite the global variable SYS_PROMPT\n",
    "            # Since this class is initialized once in the demo\n",
    "            # and the global variable will not updated after the initialization\n",
    "            SYS_PROMPT = conv_templates[conv_mode].system\n",
    "\n",
    "            # TODO: allow setting the device\n",
    "            disable_torch_init()\n",
    "            self.conv_mode = conv_mode\n",
    "            if source == \"huggingface\":\n",
    "                from huggingface_hub import snapshot_download\n",
    "                model_path = snapshot_download(model_path)\n",
    "            model_name = get_model_name_from_path(model_path)\n",
    "            self.tokenizer, self.model, self.image_processor, self.context_len = load_pretrained_model(\n",
    "                model_path, model_name\n",
    "            )\n",
    "            logger.info(f\"Model {model_name} loaded successfully. Context length: {self.context_len}\")\n",
    "        elif source == \"nim\":\n",
    "            self.base_url = \"https://api.nvcf.nvidia.com/v2/nvcf/pexec/functions/a2dec46a-b444-45aa-a1fc-a510ca41f186\"\n",
    "            if api_key == \"\":\n",
    "                api_key = os.getenv(\"api_key\", \"Invalid\")\n",
    "            if api_key == \"Invalid\":\n",
    "                raise ValueError(\"API key is not provided.\")\n",
    "            self.api_key = api_key\n",
    "        elif source == \"huggingface\":\n",
    "            pass\n",
    "        else:\n",
    "            raise NotImplementedError(f\"Source {source} is not supported.\")\n",
    "\n",
    "    def generate_response_local(\n",
    "        self,\n",
    "        messages: list = [],\n",
    "        max_tokens: int = 1024,\n",
    "        temperature: float = 0.0,\n",
    "        top_p: float = 0.9,\n",
    "        system_prompt: str | None = None,\n",
    "    ):\n",
    "        \"\"\"Generate the response\"\"\"\n",
    "        logger.debug(f\"Generating response with {len(messages)} messages\")\n",
    "\n",
    "        from llava.constants import IMAGE_TOKEN_INDEX\n",
    "        from llava.conversation import SeparatorStyle, conv_templates\n",
    "        from llava.mm_utils import KeywordsStoppingCriteria, process_images, tokenizer_image_token\n",
    "\n",
    "        images = []\n",
    "\n",
    "        conv = conv_templates[self.conv_mode].copy()\n",
    "        if system_prompt is not None:\n",
    "            conv.system = system_prompt\n",
    "        user_role = conv.roles[0]\n",
    "        assistant_role = conv.roles[1]\n",
    "\n",
    "        for message in messages:\n",
    "            role = user_role if message[\"role\"] == \"user\" else assistant_role\n",
    "            prompt = \"\"\n",
    "            for content in message[\"content\"]:\n",
    "                if content[\"type\"] == \"text\":\n",
    "                    prompt += content[\"text\"]\n",
    "                if content[\"type\"] == \"image_path\":\n",
    "                    image_paths = (\n",
    "                        content[\"image_path\"] if isinstance(content[\"image_path\"], list) else [content[\"image_path\"]]\n",
    "                    )\n",
    "                    for image_path in image_paths:\n",
    "                        images.append(load_image(image_path))\n",
    "            conv.append_message(role, prompt)\n",
    "\n",
    "        if conv.sep_style == SeparatorStyle.LLAMA_3:\n",
    "            conv.append_message(assistant_role, \"\")  # add \"\" to the assistant message\n",
    "\n",
    "        prompt_text = conv.get_prompt()\n",
    "        logger.debug(f\"Prompt input: {prompt_text}\")\n",
    "\n",
    "        if len(images) > 0:\n",
    "            images_tensor = process_images(images, self.image_processor, self.model.config).to(\n",
    "                self.model.device, dtype=torch.float16\n",
    "            )\n",
    "        images_input = [images_tensor] if len(images) > 0 else None\n",
    "\n",
    "        tokens = tokenizer_image_token(prompt_text, self.tokenizer, IMAGE_TOKEN_INDEX, return_tensors=\"pt\")\n",
    "\n",
    "        input_ids = (tokens.unsqueeze(0).to(self.model.device))\n",
    "\n",
    "        stop_str = conv.sep if conv.sep_style != SeparatorStyle.TWO else conv.sep2\n",
    "        keywords = [stop_str]\n",
    "        stopping_criteria = KeywordsStoppingCriteria(keywords, self.tokenizer, input_ids)\n",
    "\n",
    "        start_time = time.time()\n",
    "        with torch.inference_mode():\n",
    "            output_ids = self.model.generate(\n",
    "                input_ids,\n",
    "                images=images_input,\n",
    "                do_sample=True if temperature > 0 else False,\n",
    "                temperature=temperature,\n",
    "                top_p=top_p,\n",
    "                num_beams=1,\n",
    "                max_new_tokens=max_tokens,\n",
    "                use_cache=True,\n",
    "                stopping_criteria=[stopping_criteria],\n",
    "                pad_token_id=self.tokenizer.eos_token_id,\n",
    "                min_new_tokens=2,\n",
    "            )\n",
    "        end_time = time.time()\n",
    "        logger.debug(f\"Time taken to generate {len(output_ids[0])} tokens: {end_time - start_time:.2f} seconds\")\n",
    "        logger.debug(f\"Tokens per second: {len(output_ids[0]) / (end_time - start_time):.2f}\")\n",
    "\n",
    "        outputs = self.tokenizer.batch_decode(output_ids, skip_special_tokens=True)[0]\n",
    "        outputs = outputs.strip()\n",
    "        if outputs.endswith(stop_str):\n",
    "            outputs = outputs[: -len(stop_str)]\n",
    "        outputs = outputs.strip()\n",
    "        logger.debug(f\"Assistant: {outputs}\")\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    def generate_response_nim(\n",
    "        self,\n",
    "        messages: list = [],\n",
    "        max_tokens: int = 1024,\n",
    "        temperature: float = 0.0,\n",
    "        top_p: float = 0.9,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        \"\"\"Generate the response using the NIM API\"\"\"\n",
    "        logger.debug(f\"Generating response with {len(messages)} messages using the NIM API\")\n",
    "        req_messages = []\n",
    "        for message in messages:\n",
    "            role = message[\"role\"]  # expert has already been squashed into user\n",
    "            contents = []\n",
    "            for content in message[\"content\"]:\n",
    "                if content[\"type\"] == \"text\":\n",
    "                    contents.append({\"type\": \"text\", \"text\": content[\"text\"]})\n",
    "                if content[\"type\"] == \"image_path\":\n",
    "                    # if the path is cached from a URL, then use the URL\n",
    "                    if content[\"image_path\"] in self.cache_images.cache_images.values():\n",
    "                        for url, value in self.cache_images.cache_images.items():\n",
    "                            if value == content[\"image_path\"]:\n",
    "                                local_path = self.cache_images.dir()\n",
    "                                url = url.replace(local_path, REMOTE_URL)\n",
    "                                contents.append({\"type\": \"image_url\", \"image_url\":{\"url\": url}})\n",
    "                    elif os.path.exists(content[\"image_path\"]):\n",
    "                        data_url = image_to_data_url(content[\"image_path\"], max_size=(384, 384))\n",
    "                        logger.debug(f\"Length of the data URL: {len(data_url)}\")\n",
    "                        contents.append({\"type\": \"image_url\", \"image_url\": {\"url\": data_url}})\n",
    "            req_messages.append({\"role\": role, \"content\": contents})\n",
    "        logger.debug(f\"Request messages: {req_messages}\")\n",
    "        response = requests.post(\n",
    "            self.base_url,\n",
    "            headers={\n",
    "                \"Authorization\": f\"Bearer {self.api_key}\",\n",
    "                \"Accept\": \"application/json\"\n",
    "            },\n",
    "            json={\n",
    "                \"messages\": req_messages,\n",
    "                \"max_tokens\": max_tokens,\n",
    "                \"temperature\": temperature,\n",
    "                \"top_p\": top_p,\n",
    "            },\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            response.raise_for_status()\n",
    "            return response.json()[\"choices\"][0][\"message\"][\"content\"]\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to get the response from the NIM API: {e}\")\n",
    "            return f\"Sorry, I met an error: {e}.\"\n",
    "\n",
    "    def generate_response(self, **kwargs):\n",
    "        \"\"\"Generate the response\"\"\"\n",
    "        if self.source == \"local\" or self.source == \"huggingface\":\n",
    "            return self.generate_response_local(**kwargs)\n",
    "        elif self.source == \"nim\":\n",
    "            raise NotImplementedError(\"NIM API is not supported in the local demo.\")\n",
    "            # return self.generate_response_nim(**kwargs)\n",
    "        raise NotImplementedError(f\"Source {self.source} is not supported.\")\n",
    "\n",
    "    def squash_expert_messages_into_user(self, messages: list):\n",
    "        \"\"\"Squash consecutive expert messages into a single user message.\"\"\"\n",
    "        logger.debug(\"Squashing expert messages into user messages\")\n",
    "        messages = deepcopy(messages)  # Create a deep copy to avoid modifying the original list\n",
    "\n",
    "        i = 0\n",
    "        while i < len(messages):\n",
    "            if messages[i][\"role\"] == \"expert\":\n",
    "                messages[i][\"role\"] = \"user\"\n",
    "                j = i + 1\n",
    "                while j < len(messages) and messages[j][\"role\"] == \"expert\":\n",
    "                    messages[i][\"content\"].extend(messages[j][\"content\"])  # Append the content directly\n",
    "                    j += 1\n",
    "                del messages[i + 1 : j]  # Remove all the squashed expert messages\n",
    "\n",
    "            i += 1\n",
    "\n",
    "        return messages\n",
    "\n",
    "    def process_prompt(self, prompt, sv, chat_history):\n",
    "        \"\"\"Process the prompt and return the result. Inputs/outputs are the gradio components.\"\"\"\n",
    "        logger.debug(f\"Process the image and return the result\")\n",
    "\n",
    "        if sv.temp_working_dir is None:\n",
    "            sv.temp_working_dir = tempfile.mkdtemp()\n",
    "\n",
    "        if sv.modality_prompt == \"Auto\":\n",
    "            modality = get_modality(sv.image_url, text=prompt)\n",
    "        else:\n",
    "            modality = sv.modality_prompt\n",
    "        mod_msg = f\"This is a {modality} image.\\n\" if modality != \"Unknown\" else \"\"\n",
    "\n",
    "        model_cards = sv.sys_msg if sv.use_model_cards else \"\"\n",
    "\n",
    "        img_file = self.cache_images.get(sv.image_url, None, list_return=True)\n",
    "\n",
    "        if isinstance(img_file, str):\n",
    "            if \"<image>\" not in prompt:\n",
    "                _prompt = model_cards + \"<image>\" + mod_msg + prompt\n",
    "                sv.sys_msgs_to_hide.append(model_cards + \"<image>\" + mod_msg)\n",
    "            else:\n",
    "                _prompt = model_cards + mod_msg + prompt\n",
    "                if model_cards + mod_msg != \"\":\n",
    "                    sv.sys_msgs_to_hide.append(model_cards + mod_msg)\n",
    "\n",
    "            if img_file.endswith(\".nii.gz\"):  # Take the specific slice from a volume\n",
    "                chat_history.append(\n",
    "                    _prompt,\n",
    "                    image_path=os.path.join(self.cache_images.dir(), get_slice_filenames(img_file, sv.slice_index)),\n",
    "                )\n",
    "            else:\n",
    "                chat_history.append(_prompt, image_path=img_file)\n",
    "        elif isinstance(img_file, list):\n",
    "            # multi-modal images\n",
    "            prompt = (\n",
    "                prompt.replace(\"<image>\", \"\") if \"<image>\" in prompt else prompt\n",
    "            )  # remove the image token if it's in the prompt\n",
    "            special_token = \"T1(contrast enhanced): <image1>, T1: <image2>, T2: <image3>, FLAIR: <image4> \"\n",
    "            mod_msg = f\"These are different {modality} modalities.\\n\"\n",
    "            _prompt = model_cards + special_token + mod_msg + prompt\n",
    "            image_paths = [os.path.join(self.cache_images.dir(), get_slice_filenames(f, sv.slice_index)) for f in img_file]\n",
    "            chat_history.append(_prompt, image_path=image_paths)\n",
    "            sv.sys_msgs_to_hide.append(model_cards + special_token + mod_msg)\n",
    "        elif img_file is None:\n",
    "            # text-only prompt\n",
    "            chat_history.append(prompt)  # no image token\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid image file: {img_file}\")\n",
    "\n",
    "        logger.info(f\"Processing the prompt: {prompt}, with max tokens: {sv.max_tokens}, temperature: {sv.temperature}, top P: {sv.top_p}, slice index: {sv.slice_index}\")\n",
    "        outputs = self.generate_response(\n",
    "            messages=self.squash_expert_messages_into_user(chat_history.messages),\n",
    "            max_tokens=sv.max_tokens,\n",
    "            temperature=sv.temperature,\n",
    "            top_p=sv.top_p,\n",
    "            system_prompt=sv.sys_prompt,\n",
    "        )\n",
    "\n",
    "        chat_history.append(outputs, role=\"assistant\")\n",
    "\n",
    "        # check the message mentions any expert model\n",
    "        expert = None\n",
    "\n",
    "        for expert_model in [ExpertVista3D]:\n",
    "            expert = expert_model() if expert_model().mentioned_by(outputs) else None\n",
    "            if expert:\n",
    "                break\n",
    "\n",
    "        if expert:\n",
    "            logger.info(f\"Expert model {expert.__class__.__name__} is being called to process {sv.image_url}.\")\n",
    "            try:\n",
    "                if sv.image_url is None:\n",
    "                    logger.debug(\n",
    "                        \"Image URL is None. Try restoring the image URL from the backup to continue expert processing.\"\n",
    "                    )\n",
    "                    sv.restore_from_backup(\"image_url\")\n",
    "                    sv.restore_from_backup(\"slice_index\")\n",
    "                text_output, seg_image, instruction = expert.run(\n",
    "                    image_url=sv.image_url,\n",
    "                    input=outputs,\n",
    "                    output_dir=sv.temp_working_dir,\n",
    "                    img_file=self.cache_images.get(sv.image_url, None, list_return=True),\n",
    "                    slice_index=sv.slice_index,\n",
    "                    prompt=prompt,\n",
    "                )\n",
    "            except Exception as e:\n",
    "                import traceback\n",
    "                traceback.print_exc()\n",
    "                logger.debug(f\"Error: {e}\")\n",
    "                text_output = f\"Sorry I met an error: {e}\"\n",
    "                seg_image = None\n",
    "                instruction = \"\"\n",
    "\n",
    "            chat_history.append(text_output, image_path=seg_image, role=\"expert\")\n",
    "            if instruction:\n",
    "                chat_history.append(instruction, role=\"expert\")\n",
    "                outputs = self.generate_response(\n",
    "                    messages=self.squash_expert_messages_into_user(chat_history.messages),\n",
    "                    max_tokens=sv.max_tokens,\n",
    "                    temperature=sv.temperature,\n",
    "                    top_p=sv.top_p,\n",
    "                    system_prompt=sv.sys_prompt,\n",
    "                )\n",
    "                chat_history.append(outputs, role=\"assistant\")\n",
    "\n",
    "        new_sv = SessionVariables()\n",
    "        # Keep these parameters accross one conversation\n",
    "        new_sv.sys_prompt=sv.sys_prompt\n",
    "        new_sv.sys_msg=sv.sys_msg\n",
    "        new_sv.use_model_cards=sv.use_model_cards\n",
    "        new_sv.temp_working_dir=sv.temp_working_dir\n",
    "        new_sv.max_tokens=sv.max_tokens\n",
    "        new_sv.temperature=sv.temperature\n",
    "        new_sv.top_p=sv.top_p\n",
    "        # new_sv.interactive=True,\n",
    "        new_sv.sys_msgs_to_hide=sv.sys_msgs_to_hide\n",
    "        new_sv.backup={\"image_url\": sv.image_url, \"slice_index\": sv.slice_index},\n",
    "\n",
    "        return (\n",
    "            new_sv,\n",
    "            chat_history,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cache images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 75/75 [00:00<00:00, 113687.31it/s]\n"
     ]
    }
   ],
   "source": [
    "LIVER_URL = \"https://developer.download.nvidia.com/assets/Clara/monai/samples/ct_liver_0.nii.gz\"\n",
    "\n",
    "cache_dir = \"./data\"\n",
    "cache_images = ImageCache(cache_dir)\n",
    "\n",
    "os.makedirs(cache_dir, exist_ok=True)\n",
    "cache_images.cache({\"Sample 1\": LIVER_URL})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment(\n",
    "        prompts,\n",
    "        seg_token=\"<segmentation>\",\n",
    "        model=\"MONAI/Llama3-VILA-M3-8B\",\n",
    "        conv_mode=\"llama_3\",\n",
    "        image_url=LIVER_URL,\n",
    "        slice_index=57):\n",
    "    global SEGMENTATION_TOKEN\n",
    "    SEGMENTATION_TOKEN = seg_token\n",
    "    if not isinstance(prompts, list):\n",
    "        prompts = [prompts]\n",
    "    sv = SessionVariables()\n",
    "    m3 = M3Generator(cache_images, source=\"huggingface\", model_path=model, conv_mode=conv_mode)\n",
    "\n",
    "    sv.image_url = image_url\n",
    "    sv.slice_index = slice_index\n",
    "\n",
    "    chat_history = ChatHistory()\n",
    "    for prompt in prompts:\n",
    "        sv, chat_history = m3.process_prompt(prompt, sv, chat_history)\n",
    "\n",
    "    print(\"=\"*100)\n",
    "    for message in chat_history.messages:\n",
    "        role = message[\"role\"].upper()\n",
    "        content = message[\"content\"]\n",
    "        print(f\"{role}: {content}\")\n",
    "\n",
    "    print(\"=\"*100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment with 8B model\n",
    "\n",
    "We can put multiple turn prompt in a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 21 files: 100%|| 21/21 [00:00<00:00, 117284.13it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading checkpoint shards: 100%|| 4/4 [00:09<00:00,  2.48s/it]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Model df60e0276e2ae10624c86dabe909847a03b2a5cb loaded successfully. Context length: 2048\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-03-02 07:01:25,064 - __main__ - INFO - Model df60e0276e2ae10624c86dabe909847a03b2a5cb loaded successfully. Context length: 2048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing the prompt: Can you identify any liver masses or tumors?, with max tokens: 1024, temperature: 0.0, top P: 0.9, slice index: 57\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-03-02 07:01:25,066 - __main__ - INFO - Processing the prompt: Can you identify any liver masses or tumors?, with max tokens: 1024, temperature: 0.0, top P: 0.9, slice index: 57\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:392: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:397: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Expert model ExpertVista3D is being called to process https://developer.download.nvidia.com/assets/Clara/monai/samples/ct_liver_0.nii.gz.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-03-02 07:01:25,914 - __main__ - INFO - Expert model ExpertVista3D is being called to process https://developer.download.nvidia.com/assets/Clara/monai/samples/ct_liver_0.nii.gz.\n",
      "2025-03-02 07:01:25,919 - INFO - --- input summary of monai.bundle.scripts.run ---\n",
      "2025-03-02 07:01:25,925 - INFO - > config_file: '/root/.cache/torch/hub/bundle/vista3d_v0.5.4/vista3d/configs/inference.json'\n",
      "2025-03-02 07:01:25,928 - INFO - > workflow_type: 'infer'\n",
      "2025-03-02 07:01:25,929 - INFO - > bundle_root: '/root/.cache/torch/hub/bundle/vista3d_v0.5.4/vista3d'\n",
      "2025-03-02 07:01:25,930 - INFO - > logging_file: '/root/.cache/torch/hub/bundle/vista3d_v0.5.4/vista3d/configs/logging.conf'\n",
      "2025-03-02 07:01:25,930 - INFO - > meta_file: '/root/.cache/torch/hub/bundle/vista3d_v0.5.4/vista3d/configs/metadata.json'\n",
      "2025-03-02 07:01:25,931 - INFO - > input_dict: {'image': 'data/ct_liver_0.nii.gz', 'label_prompt': [1, 26]}\n",
      "2025-03-02 07:01:25,931 - INFO - > output_dtype: 'uint8'\n",
      "2025-03-02 07:01:25,936 - INFO - > separate_folder: False\n",
      "2025-03-02 07:01:25,937 - INFO - > output_ext: '.nii.gz'\n",
      "2025-03-02 07:01:25,937 - INFO - > output_dir: '/tmp/tmpnlk3tfjx'\n",
      "2025-03-02 07:01:25,938 - INFO - ---\n",
      "\n",
      "\n",
      "2025-03-02 07:01:25,938 - INFO - Setting logging properties based on config: /root/.cache/torch/hub/bundle/vista3d_v0.5.4/vista3d/configs/logging.conf.\n",
      "2025-03-02 07:01:27,987 - root - INFO - Restored all variables from /root/.cache/torch/hub/bundle/vista3d_v0.5.4/vista3d/models/model.pt\n",
      "2025-03-02 07:01:27,990 - ignite.engine.engine.Vista3dEvaluator - INFO - Engine run resuming from iteration 0, epoch 0 until 1 epochs\n",
      "2025-03-02 07:01:30,050 INFO image_writer.py:197 - writing: /tmp/tmpnlk3tfjx/ct_liver_0_trans.nii.gz\n",
      "2025-03-02 07:01:30,337 - ignite.engine.engine.Vista3dEvaluator - INFO - Epoch[1] Complete. Time taken: 00:00:02.346\n",
      "2025-03-02 07:01:30,338 - ignite.engine.engine.Vista3dEvaluator - INFO - Engine run complete. Time taken: 00:00:02.346\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:392: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:397: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Processing the prompt: Describe the image in detail., with max tokens: 1024, temperature: 0.0, top P: 0.9, slice index: None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-03-02 07:01:31,552 - __main__ - INFO - Processing the prompt: Describe the image in detail., with max tokens: 1024, temperature: 0.0, top P: 0.9, slice index: None\n",
      "====================================================================================================\n",
      "USER: [{'type': 'text', 'text': \"Here is a list of available expert models:\\n<BRATS(args)> Modality: MRI, Task: segmentation, Overview: A pre-trained model for volumetric (3D) segmentation of brain tumor subregions from multimodal MRIs based on BraTS 2018 data, Accuracy: Tumor core (TC): 0.8559 - Whole tumor (WT): 0.9026 - Enhancing tumor (ET): 0.7905 - Average: 0.8518, Valid args are: None\\n<VISTA3D(args)> Modality: CT, Task: segmentation, Overview: domain-specialized interactive foundation model developed for segmenting and annotating human anatomies with precision, Accuracy: 127 organs: 0.792 Dice on average, Valid args are: 'everything', 'hepatic tumor', 'pancreatic tumor', 'lung tumor', 'bone lesion', 'organs', 'cardiovascular', 'gastrointestinal', 'skeleton', or 'muscles'\\n<VISTA2D(args)> Modality: cell imaging, Task: segmentation, Overview: model for cell segmentation, which was trained on a variety of cell imaging outputs, including brightfield, phase-contrast, fluorescence, confocal, or electron microscopy, Accuracy: Good accuracy across several cell imaging datasets, Valid args are: None\\n<CXR(args)> Modality: chest x-ray (CXR), Task: classification, Overview: pre-trained model which are trained on large cohorts of data, Accuracy: Good accuracy across several diverse chest x-rays datasets, Valid args are: None\\nGive the model <NAME(args)> when selecting a suitable expert model.\\n<image>This is a CT image.\\nCan you identify any liver masses or tumors?\"}, {'type': 'image_path', 'image_path': 'data/ct_liver_0_slice57_img.jpg'}]\n",
      "ASSISTANT: [{'type': 'text', 'text': 'This looks like a CT image. Let me trigger <VISTA3D(hepatic tumor)>.'}]\n",
      "EXPERT: [{'type': 'text', 'text': 'The results are <segmentation>. The colors in this image describe red: liver, blue: hepatic tumor. '}, {'type': 'image_path', 'image_path': '/tmp/tmpx6klsy41/seg_aec8a73d-d726-4dc4-8e90-a865f768cca6.jpg'}]\n",
      "EXPERT: [{'type': 'text', 'text': 'Use this result to respond to this prompt:\\nCan you identify any liver masses or tumors?'}]\n",
      "ASSISTANT: [{'type': 'text', 'text': 'yes'}]\n",
      "USER: [{'type': 'text', 'text': 'Describe the image in detail.'}]\n",
      "ASSISTANT: [{'type': 'text', 'text': 'The scan indicates high grade glioma.'}]\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "experiment([\"Can you identify any liver masses or tumors?\", \"Describe the image in detail.\"], seg_token=\"<segmentation>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 21 files: 100%|| 21/21 [00:00<00:00, 94002.54it/s]\n",
      "Loading checkpoint shards: 100%|| 4/4 [00:09<00:00,  2.43s/it]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Model df60e0276e2ae10624c86dabe909847a03b2a5cb loaded successfully. Context length: 2048\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-03-02 07:02:01,126 - __main__ - INFO - Model df60e0276e2ae10624c86dabe909847a03b2a5cb loaded successfully. Context length: 2048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing the prompt: Can you identify any liver masses or tumors?, with max tokens: 1024, temperature: 0.0, top P: 0.9, slice index: 57\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-03-02 07:02:01,129 - __main__ - INFO - Processing the prompt: Can you identify any liver masses or tumors?, with max tokens: 1024, temperature: 0.0, top P: 0.9, slice index: 57\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:392: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:397: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Expert model ExpertVista3D is being called to process https://developer.download.nvidia.com/assets/Clara/monai/samples/ct_liver_0.nii.gz.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-03-02 07:02:01,952 - __main__ - INFO - Expert model ExpertVista3D is being called to process https://developer.download.nvidia.com/assets/Clara/monai/samples/ct_liver_0.nii.gz.\n",
      "2025-03-02 07:02:01,954 - INFO - --- input summary of monai.bundle.scripts.run ---\n",
      "2025-03-02 07:02:01,955 - INFO - > config_file: '/root/.cache/torch/hub/bundle/vista3d_v0.5.4/vista3d/configs/inference.json'\n",
      "2025-03-02 07:02:01,955 - INFO - > workflow_type: 'infer'\n",
      "2025-03-02 07:02:01,956 - INFO - > bundle_root: '/root/.cache/torch/hub/bundle/vista3d_v0.5.4/vista3d'\n",
      "2025-03-02 07:02:01,957 - INFO - > logging_file: '/root/.cache/torch/hub/bundle/vista3d_v0.5.4/vista3d/configs/logging.conf'\n",
      "2025-03-02 07:02:01,957 - INFO - > meta_file: '/root/.cache/torch/hub/bundle/vista3d_v0.5.4/vista3d/configs/metadata.json'\n",
      "2025-03-02 07:02:01,958 - INFO - > input_dict: {'image': 'data/ct_liver_0.nii.gz', 'label_prompt': [1, 26]}\n",
      "2025-03-02 07:02:01,958 - INFO - > output_dtype: 'uint8'\n",
      "2025-03-02 07:02:01,959 - INFO - > separate_folder: False\n",
      "2025-03-02 07:02:01,959 - INFO - > output_ext: '.nii.gz'\n",
      "2025-03-02 07:02:01,959 - INFO - > output_dir: '/tmp/tmpqziogllr'\n",
      "2025-03-02 07:02:01,960 - INFO - ---\n",
      "\n",
      "\n",
      "2025-03-02 07:02:01,960 - INFO - Setting logging properties based on config: /root/.cache/torch/hub/bundle/vista3d_v0.5.4/vista3d/configs/logging.conf.\n",
      "2025-03-02 07:02:03,834 - root - INFO - Restored all variables from /root/.cache/torch/hub/bundle/vista3d_v0.5.4/vista3d/models/model.pt\n",
      "2025-03-02 07:02:03,836 - ignite.engine.engine.Vista3dEvaluator - INFO - Engine run resuming from iteration 0, epoch 0 until 1 epochs\n",
      "2025-03-02 07:02:05,940 INFO image_writer.py:197 - writing: /tmp/tmpqziogllr/ct_liver_0_trans.nii.gz\n",
      "2025-03-02 07:02:06,247 - ignite.engine.engine.Vista3dEvaluator - INFO - Epoch[1] Complete. Time taken: 00:00:02.411\n",
      "2025-03-02 07:02:06,250 - ignite.engine.engine.Vista3dEvaluator - INFO - Engine run complete. Time taken: 00:00:02.413\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:392: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:397: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Processing the prompt: Please provide a detailed description of the image., with max tokens: 1024, temperature: 0.0, top P: 0.9, slice index: None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-03-02 07:02:07,580 - __main__ - INFO - Processing the prompt: Please provide a detailed description of the image., with max tokens: 1024, temperature: 0.0, top P: 0.9, slice index: None\n",
      "====================================================================================================\n",
      "USER: [{'type': 'text', 'text': \"Here is a list of available expert models:\\n<BRATS(args)> Modality: MRI, Task: segmentation, Overview: A pre-trained model for volumetric (3D) segmentation of brain tumor subregions from multimodal MRIs based on BraTS 2018 data, Accuracy: Tumor core (TC): 0.8559 - Whole tumor (WT): 0.9026 - Enhancing tumor (ET): 0.7905 - Average: 0.8518, Valid args are: None\\n<VISTA3D(args)> Modality: CT, Task: segmentation, Overview: domain-specialized interactive foundation model developed for segmenting and annotating human anatomies with precision, Accuracy: 127 organs: 0.792 Dice on average, Valid args are: 'everything', 'hepatic tumor', 'pancreatic tumor', 'lung tumor', 'bone lesion', 'organs', 'cardiovascular', 'gastrointestinal', 'skeleton', or 'muscles'\\n<VISTA2D(args)> Modality: cell imaging, Task: segmentation, Overview: model for cell segmentation, which was trained on a variety of cell imaging outputs, including brightfield, phase-contrast, fluorescence, confocal, or electron microscopy, Accuracy: Good accuracy across several cell imaging datasets, Valid args are: None\\n<CXR(args)> Modality: chest x-ray (CXR), Task: classification, Overview: pre-trained model which are trained on large cohorts of data, Accuracy: Good accuracy across several diverse chest x-rays datasets, Valid args are: None\\nGive the model <NAME(args)> when selecting a suitable expert model.\\n<image>This is a CT image.\\nCan you identify any liver masses or tumors?\"}, {'type': 'image_path', 'image_path': 'data/ct_liver_0_slice57_img.jpg'}]\n",
      "ASSISTANT: [{'type': 'text', 'text': 'This looks like a CT image. Let me trigger <VISTA3D(hepatic tumor)>.'}]\n",
      "EXPERT: [{'type': 'text', 'text': 'The results are <image>. The colors in this image describe red: liver, blue: hepatic tumor. '}, {'type': 'image_path', 'image_path': '/tmp/tmpt9h9zewk/seg_7659d9b6-b086-4820-83ea-8fae0d064f25.jpg'}]\n",
      "EXPERT: [{'type': 'text', 'text': 'Use this result to respond to this prompt:\\nCan you identify any liver masses or tumors?'}]\n",
      "ASSISTANT: [{'type': 'text', 'text': 'yes'}]\n",
      "USER: [{'type': 'text', 'text': 'Please provide a detailed description of the image.'}]\n",
      "ASSISTANT: [{'type': 'text', 'text': 'The scan indicates high grade glioma.'}]\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "experiment([\"Can you identify any liver masses or tumors?\", \"Please provide a detailed description of the image.\"], seg_token=\"<image>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 21 files: 100%|| 21/21 [00:00<00:00, 98523.92it/s]\n",
      "Loading checkpoint shards: 100%|| 4/4 [00:09<00:00,  2.25s/it]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Model df60e0276e2ae10624c86dabe909847a03b2a5cb loaded successfully. Context length: 2048\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-03-02 07:05:57,889 - __main__ - INFO - Model df60e0276e2ae10624c86dabe909847a03b2a5cb loaded successfully. Context length: 2048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing the prompt: Is there any abnormality in the image?, with max tokens: 1024, temperature: 0.0, top P: 0.9, slice index: 57\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-03-02 07:05:57,891 - __main__ - INFO - Processing the prompt: Is there any abnormality in the image?, with max tokens: 1024, temperature: 0.0, top P: 0.9, slice index: 57\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Expert model ExpertVista3D is being called to process https://developer.download.nvidia.com/assets/Clara/monai/samples/ct_liver_0.nii.gz.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-03-02 07:05:58,649 - __main__ - INFO - Expert model ExpertVista3D is being called to process https://developer.download.nvidia.com/assets/Clara/monai/samples/ct_liver_0.nii.gz.\n",
      "2025-03-02 07:05:58,651 - INFO - --- input summary of monai.bundle.scripts.run ---\n",
      "2025-03-02 07:05:58,652 - INFO - > config_file: '/root/.cache/torch/hub/bundle/vista3d_v0.5.4/vista3d/configs/inference.json'\n",
      "2025-03-02 07:05:58,652 - INFO - > workflow_type: 'infer'\n",
      "2025-03-02 07:05:58,653 - INFO - > bundle_root: '/root/.cache/torch/hub/bundle/vista3d_v0.5.4/vista3d'\n",
      "2025-03-02 07:05:58,653 - INFO - > logging_file: '/root/.cache/torch/hub/bundle/vista3d_v0.5.4/vista3d/configs/logging.conf'\n",
      "2025-03-02 07:05:58,653 - INFO - > meta_file: '/root/.cache/torch/hub/bundle/vista3d_v0.5.4/vista3d/configs/metadata.json'\n",
      "2025-03-02 07:05:58,654 - INFO - > input_dict: {'image': 'data/ct_liver_0.nii.gz'}\n",
      "2025-03-02 07:05:58,654 - INFO - > output_dtype: 'uint8'\n",
      "2025-03-02 07:05:58,655 - INFO - > separate_folder: False\n",
      "2025-03-02 07:05:58,655 - INFO - > output_ext: '.nii.gz'\n",
      "2025-03-02 07:05:58,655 - INFO - > output_dir: '/tmp/tmpp4po79lg'\n",
      "2025-03-02 07:05:58,656 - INFO - ---\n",
      "\n",
      "\n",
      "2025-03-02 07:05:58,657 - INFO - Setting logging properties based on config: /root/.cache/torch/hub/bundle/vista3d_v0.5.4/vista3d/configs/logging.conf.\n",
      "2025-03-02 07:06:00,492 - root - INFO - Restored all variables from /root/.cache/torch/hub/bundle/vista3d_v0.5.4/vista3d/models/model.pt\n",
      "2025-03-02 07:06:00,495 - ignite.engine.engine.Vista3dEvaluator - INFO - Engine run resuming from iteration 0, epoch 0 until 1 epochs\n",
      "2025-03-02 07:06:03,193 INFO image_writer.py:197 - writing: /tmp/tmpp4po79lg/ct_liver_0_trans.nii.gz\n",
      "2025-03-02 07:06:03,566 - ignite.engine.engine.Vista3dEvaluator - INFO - Epoch[1] Complete. Time taken: 00:00:03.070\n",
      "2025-03-02 07:06:03,569 - ignite.engine.engine.Vista3dEvaluator - INFO - Engine run complete. Time taken: 00:00:03.073\n",
      "====================================================================================================\n",
      "USER: [{'type': 'text', 'text': \"Here is a list of available expert models:\\n<BRATS(args)> Modality: MRI, Task: segmentation, Overview: A pre-trained model for volumetric (3D) segmentation of brain tumor subregions from multimodal MRIs based on BraTS 2018 data, Accuracy: Tumor core (TC): 0.8559 - Whole tumor (WT): 0.9026 - Enhancing tumor (ET): 0.7905 - Average: 0.8518, Valid args are: None\\n<VISTA3D(args)> Modality: CT, Task: segmentation, Overview: domain-specialized interactive foundation model developed for segmenting and annotating human anatomies with precision, Accuracy: 127 organs: 0.792 Dice on average, Valid args are: 'everything', 'hepatic tumor', 'pancreatic tumor', 'lung tumor', 'bone lesion', 'organs', 'cardiovascular', 'gastrointestinal', 'skeleton', or 'muscles'\\n<VISTA2D(args)> Modality: cell imaging, Task: segmentation, Overview: model for cell segmentation, which was trained on a variety of cell imaging outputs, including brightfield, phase-contrast, fluorescence, confocal, or electron microscopy, Accuracy: Good accuracy across several cell imaging datasets, Valid args are: None\\n<CXR(args)> Modality: chest x-ray (CXR), Task: classification, Overview: pre-trained model which are trained on large cohorts of data, Accuracy: Good accuracy across several diverse chest x-rays datasets, Valid args are: None\\nGive the model <NAME(args)> when selecting a suitable expert model.\\n<image>This is a CT image.\\nIs there any abnormality in the image?\"}, {'type': 'image_path', 'image_path': 'data/ct_liver_0_slice57_img.jpg'}]\n",
      "ASSISTANT: [{'type': 'text', 'text': 'This looks like a CT image. Let me trigger <VISTA3D(everything)>.'}]\n",
      "EXPERT: [{'type': 'text', 'text': 'The results are <segmentation>. The colors in this image describe red: liver, blue: spleen, yellow: pancreas, magenta: aorta, green: inferior vena cava, indigo: right adrenal gland, darkorange: left adrenal gland, cyan: stomach, pink: left kidney, brown: portal vein and splenic vein, orange: vertebrae L2, lime: vertebrae L1, orange: left rib 9, gold: left rib 10, yellowgreen: left rib 11, darkgreen: left rib 12, red: right rib 10, blue: right rib 11, yellow: right rib 12, magenta: left autochthon, green: right autochthon, indigo: left iliopsoas, darkorange: right iliopsoas, cyan: costal cartilages, pink: prostate, brown: spinal cord. '}, {'type': 'image_path', 'image_path': '/tmp/tmpv3jhw74b/seg_1607dc13-58cb-4173-bfb2-e2ef2eb2cfda.jpg'}]\n",
      "EXPERT: [{'type': 'text', 'text': 'Use this result to respond to this prompt:\\nIs there any abnormality in the image?'}]\n",
      "ASSISTANT: [{'type': 'text', 'text': 'no'}]\n",
      "====================================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:392: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:397: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "experiment(\"Is there any abnormality in the image?\", seg_token=\"<segmentation>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 21 files: 100%|| 21/21 [00:00<00:00, 111212.61it/s]\n",
      "Loading checkpoint shards: 100%|| 4/4 [00:08<00:00,  2.22s/it]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Model df60e0276e2ae10624c86dabe909847a03b2a5cb loaded successfully. Context length: 2048\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-03-02 07:06:27,507 - __main__ - INFO - Model df60e0276e2ae10624c86dabe909847a03b2a5cb loaded successfully. Context length: 2048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing the prompt: Is there any abnormality in the image?, with max tokens: 1024, temperature: 0.0, top P: 0.9, slice index: 57\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-03-02 07:06:27,509 - __main__ - INFO - Processing the prompt: Is there any abnormality in the image?, with max tokens: 1024, temperature: 0.0, top P: 0.9, slice index: 57\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Expert model ExpertVista3D is being called to process https://developer.download.nvidia.com/assets/Clara/monai/samples/ct_liver_0.nii.gz.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-03-02 07:06:28,289 - __main__ - INFO - Expert model ExpertVista3D is being called to process https://developer.download.nvidia.com/assets/Clara/monai/samples/ct_liver_0.nii.gz.\n",
      "2025-03-02 07:06:28,291 - INFO - --- input summary of monai.bundle.scripts.run ---\n",
      "2025-03-02 07:06:28,292 - INFO - > config_file: '/root/.cache/torch/hub/bundle/vista3d_v0.5.4/vista3d/configs/inference.json'\n",
      "2025-03-02 07:06:28,292 - INFO - > workflow_type: 'infer'\n",
      "2025-03-02 07:06:28,293 - INFO - > bundle_root: '/root/.cache/torch/hub/bundle/vista3d_v0.5.4/vista3d'\n",
      "2025-03-02 07:06:28,293 - INFO - > logging_file: '/root/.cache/torch/hub/bundle/vista3d_v0.5.4/vista3d/configs/logging.conf'\n",
      "2025-03-02 07:06:28,293 - INFO - > meta_file: '/root/.cache/torch/hub/bundle/vista3d_v0.5.4/vista3d/configs/metadata.json'\n",
      "2025-03-02 07:06:28,294 - INFO - > input_dict: {'image': 'data/ct_liver_0.nii.gz'}\n",
      "2025-03-02 07:06:28,295 - INFO - > output_dtype: 'uint8'\n",
      "2025-03-02 07:06:28,295 - INFO - > separate_folder: False\n",
      "2025-03-02 07:06:28,296 - INFO - > output_ext: '.nii.gz'\n",
      "2025-03-02 07:06:28,296 - INFO - > output_dir: '/tmp/tmp809wj7n1'\n",
      "2025-03-02 07:06:28,296 - INFO - ---\n",
      "\n",
      "\n",
      "2025-03-02 07:06:28,301 - INFO - Setting logging properties based on config: /root/.cache/torch/hub/bundle/vista3d_v0.5.4/vista3d/configs/logging.conf.\n",
      "2025-03-02 07:06:30,216 - root - INFO - Restored all variables from /root/.cache/torch/hub/bundle/vista3d_v0.5.4/vista3d/models/model.pt\n",
      "2025-03-02 07:06:30,217 - ignite.engine.engine.Vista3dEvaluator - INFO - Engine run resuming from iteration 0, epoch 0 until 1 epochs\n",
      "2025-03-02 07:06:32,874 INFO image_writer.py:197 - writing: /tmp/tmp809wj7n1/ct_liver_0_trans.nii.gz\n",
      "2025-03-02 07:06:33,215 - ignite.engine.engine.Vista3dEvaluator - INFO - Epoch[1] Complete. Time taken: 00:00:02.991\n",
      "2025-03-02 07:06:33,217 - ignite.engine.engine.Vista3dEvaluator - INFO - Engine run complete. Time taken: 00:00:02.992\n",
      "====================================================================================================\n",
      "USER: [{'type': 'text', 'text': \"Here is a list of available expert models:\\n<BRATS(args)> Modality: MRI, Task: segmentation, Overview: A pre-trained model for volumetric (3D) segmentation of brain tumor subregions from multimodal MRIs based on BraTS 2018 data, Accuracy: Tumor core (TC): 0.8559 - Whole tumor (WT): 0.9026 - Enhancing tumor (ET): 0.7905 - Average: 0.8518, Valid args are: None\\n<VISTA3D(args)> Modality: CT, Task: segmentation, Overview: domain-specialized interactive foundation model developed for segmenting and annotating human anatomies with precision, Accuracy: 127 organs: 0.792 Dice on average, Valid args are: 'everything', 'hepatic tumor', 'pancreatic tumor', 'lung tumor', 'bone lesion', 'organs', 'cardiovascular', 'gastrointestinal', 'skeleton', or 'muscles'\\n<VISTA2D(args)> Modality: cell imaging, Task: segmentation, Overview: model for cell segmentation, which was trained on a variety of cell imaging outputs, including brightfield, phase-contrast, fluorescence, confocal, or electron microscopy, Accuracy: Good accuracy across several cell imaging datasets, Valid args are: None\\n<CXR(args)> Modality: chest x-ray (CXR), Task: classification, Overview: pre-trained model which are trained on large cohorts of data, Accuracy: Good accuracy across several diverse chest x-rays datasets, Valid args are: None\\nGive the model <NAME(args)> when selecting a suitable expert model.\\n<image>This is a CT image.\\nIs there any abnormality in the image?\"}, {'type': 'image_path', 'image_path': 'data/ct_liver_0_slice57_img.jpg'}]\n",
      "ASSISTANT: [{'type': 'text', 'text': 'This looks like a CT image. Let me trigger <VISTA3D(everything)>.'}]\n",
      "EXPERT: [{'type': 'text', 'text': 'The results are <image>. The colors in this image describe red: liver, blue: spleen, yellow: pancreas, magenta: aorta, green: inferior vena cava, indigo: right adrenal gland, darkorange: left adrenal gland, cyan: stomach, pink: left kidney, brown: portal vein and splenic vein, orange: vertebrae L2, lime: vertebrae L1, orange: left rib 9, gold: left rib 10, yellowgreen: left rib 11, darkgreen: left rib 12, red: right rib 10, blue: right rib 11, yellow: right rib 12, magenta: left autochthon, green: right autochthon, indigo: left iliopsoas, darkorange: right iliopsoas, cyan: costal cartilages, pink: prostate, brown: spinal cord. '}, {'type': 'image_path', 'image_path': '/tmp/tmp6nduvdyb/seg_0e039449-6ae2-4d33-81a3-ba430c2009bb.jpg'}]\n",
      "EXPERT: [{'type': 'text', 'text': 'Use this result to respond to this prompt:\\nIs there any abnormality in the image?'}]\n",
      "ASSISTANT: [{'type': 'text', 'text': 'no'}]\n",
      "====================================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:392: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:397: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "experiment(\"Is there any abnormality in the image?\", seg_token=\"<image>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment with 13B model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 23 files: 100%|| 23/23 [00:00<00:00, 91266.79it/s]\n",
      "Loading checkpoint shards: 100%|| 6/6 [00:12<00:00,  2.12s/it]\n",
      "Model 4e68cefd00fa5d9bb309484307cdfc9dec55785f loaded successfully. Context length: 2048\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-03-02 07:07:04,371 - __main__ - INFO - Model 4e68cefd00fa5d9bb309484307cdfc9dec55785f loaded successfully. Context length: 2048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing the prompt: Can you identify any liver masses or tumors?, with max tokens: 1024, temperature: 0.0, top P: 0.9, slice index: 57\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-03-02 07:07:04,374 - __main__ - INFO - Processing the prompt: Can you identify any liver masses or tumors?, with max tokens: 1024, temperature: 0.0, top P: 0.9, slice index: 57\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1295: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
      "  warnings.warn(\n",
      "Expert model ExpertVista3D is being called to process https://developer.download.nvidia.com/assets/Clara/monai/samples/ct_liver_0.nii.gz.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-03-02 07:07:05,258 - __main__ - INFO - Expert model ExpertVista3D is being called to process https://developer.download.nvidia.com/assets/Clara/monai/samples/ct_liver_0.nii.gz.\n",
      "2025-03-02 07:07:05,262 - INFO - --- input summary of monai.bundle.scripts.run ---\n",
      "2025-03-02 07:07:05,263 - INFO - > config_file: '/root/.cache/torch/hub/bundle/vista3d_v0.5.4/vista3d/configs/inference.json'\n",
      "2025-03-02 07:07:05,263 - INFO - > workflow_type: 'infer'\n",
      "2025-03-02 07:07:05,264 - INFO - > bundle_root: '/root/.cache/torch/hub/bundle/vista3d_v0.5.4/vista3d'\n",
      "2025-03-02 07:07:05,264 - INFO - > logging_file: '/root/.cache/torch/hub/bundle/vista3d_v0.5.4/vista3d/configs/logging.conf'\n",
      "2025-03-02 07:07:05,265 - INFO - > meta_file: '/root/.cache/torch/hub/bundle/vista3d_v0.5.4/vista3d/configs/metadata.json'\n",
      "2025-03-02 07:07:05,265 - INFO - > input_dict: {'image': 'data/ct_liver_0.nii.gz', 'label_prompt': [1, 26]}\n",
      "2025-03-02 07:07:05,266 - INFO - > output_dtype: 'uint8'\n",
      "2025-03-02 07:07:05,266 - INFO - > separate_folder: False\n",
      "2025-03-02 07:07:05,267 - INFO - > output_ext: '.nii.gz'\n",
      "2025-03-02 07:07:05,267 - INFO - > output_dir: '/tmp/tmp38l15_2u'\n",
      "2025-03-02 07:07:05,267 - INFO - ---\n",
      "\n",
      "\n",
      "2025-03-02 07:07:05,268 - INFO - Setting logging properties based on config: /root/.cache/torch/hub/bundle/vista3d_v0.5.4/vista3d/configs/logging.conf.\n",
      "2025-03-02 07:07:07,114 - root - INFO - Restored all variables from /root/.cache/torch/hub/bundle/vista3d_v0.5.4/vista3d/models/model.pt\n",
      "2025-03-02 07:07:07,115 - ignite.engine.engine.Vista3dEvaluator - INFO - Engine run resuming from iteration 0, epoch 0 until 1 epochs\n",
      "2025-03-02 07:07:09,187 INFO image_writer.py:197 - writing: /tmp/tmp38l15_2u/ct_liver_0_trans.nii.gz\n",
      "2025-03-02 07:07:09,511 - ignite.engine.engine.Vista3dEvaluator - INFO - Epoch[1] Complete. Time taken: 00:00:02.396\n",
      "2025-03-02 07:07:09,517 - ignite.engine.engine.Vista3dEvaluator - INFO - Engine run complete. Time taken: 00:00:02.402\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:392: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:397: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "USER: [{'type': 'text', 'text': \"Here is a list of available expert models:\\n<BRATS(args)> Modality: MRI, Task: segmentation, Overview: A pre-trained model for volumetric (3D) segmentation of brain tumor subregions from multimodal MRIs based on BraTS 2018 data, Accuracy: Tumor core (TC): 0.8559 - Whole tumor (WT): 0.9026 - Enhancing tumor (ET): 0.7905 - Average: 0.8518, Valid args are: None\\n<VISTA3D(args)> Modality: CT, Task: segmentation, Overview: domain-specialized interactive foundation model developed for segmenting and annotating human anatomies with precision, Accuracy: 127 organs: 0.792 Dice on average, Valid args are: 'everything', 'hepatic tumor', 'pancreatic tumor', 'lung tumor', 'bone lesion', 'organs', 'cardiovascular', 'gastrointestinal', 'skeleton', or 'muscles'\\n<VISTA2D(args)> Modality: cell imaging, Task: segmentation, Overview: model for cell segmentation, which was trained on a variety of cell imaging outputs, including brightfield, phase-contrast, fluorescence, confocal, or electron microscopy, Accuracy: Good accuracy across several cell imaging datasets, Valid args are: None\\n<CXR(args)> Modality: chest x-ray (CXR), Task: classification, Overview: pre-trained model which are trained on large cohorts of data, Accuracy: Good accuracy across several diverse chest x-rays datasets, Valid args are: None\\nGive the model <NAME(args)> when selecting a suitable expert model.\\n<image>This is a CT image.\\nCan you identify any liver masses or tumors?\"}, {'type': 'image_path', 'image_path': 'data/ct_liver_0_slice57_img.jpg'}]\n",
      "ASSISTANT: [{'type': 'text', 'text': '<VISTA3D(hepatic tumor)>hepatic tumor'}]\n",
      "EXPERT: [{'type': 'text', 'text': 'The results are <segmentation>. The colors in this image describe red: liver, blue: hepatic tumor. '}, {'type': 'image_path', 'image_path': '/tmp/tmppd6v9z7t/seg_cdf1076e-d552-47a9-bb5e-225434c8f476.jpg'}]\n",
      "EXPERT: [{'type': 'text', 'text': 'Use this result to respond to this prompt:\\nCan you identify any liver masses or tumors?'}]\n",
      "ASSISTANT: [{'type': 'text', 'text': 'yes<br>no'}]\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "experiment(\"Can you identify any liver masses or tumors?\", seg_token=\"<segmentation>\", model=\"MONAI/Llama3-VILA-M3-13B\", conv_mode=\"vicuna_v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 23 files: 100%|| 23/23 [00:00<00:00, 54226.53it/s]\n",
      "Loading checkpoint shards: 100%|| 6/6 [00:12<00:00,  2.03s/it]\n",
      "Model 4e68cefd00fa5d9bb309484307cdfc9dec55785f loaded successfully. Context length: 2048\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-03-02 07:07:35,379 - __main__ - INFO - Model 4e68cefd00fa5d9bb309484307cdfc9dec55785f loaded successfully. Context length: 2048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing the prompt: Can you identify any liver masses or tumors?, with max tokens: 1024, temperature: 0.0, top P: 0.9, slice index: 57\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-03-02 07:07:35,381 - __main__ - INFO - Processing the prompt: Can you identify any liver masses or tumors?, with max tokens: 1024, temperature: 0.0, top P: 0.9, slice index: 57\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1295: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
      "  warnings.warn(\n",
      "Expert model ExpertVista3D is being called to process https://developer.download.nvidia.com/assets/Clara/monai/samples/ct_liver_0.nii.gz.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-03-02 07:07:36,255 - __main__ - INFO - Expert model ExpertVista3D is being called to process https://developer.download.nvidia.com/assets/Clara/monai/samples/ct_liver_0.nii.gz.\n",
      "2025-03-02 07:07:36,259 - INFO - --- input summary of monai.bundle.scripts.run ---\n",
      "2025-03-02 07:07:36,263 - INFO - > config_file: '/root/.cache/torch/hub/bundle/vista3d_v0.5.4/vista3d/configs/inference.json'\n",
      "2025-03-02 07:07:36,263 - INFO - > workflow_type: 'infer'\n",
      "2025-03-02 07:07:36,263 - INFO - > bundle_root: '/root/.cache/torch/hub/bundle/vista3d_v0.5.4/vista3d'\n",
      "2025-03-02 07:07:36,264 - INFO - > logging_file: '/root/.cache/torch/hub/bundle/vista3d_v0.5.4/vista3d/configs/logging.conf'\n",
      "2025-03-02 07:07:36,264 - INFO - > meta_file: '/root/.cache/torch/hub/bundle/vista3d_v0.5.4/vista3d/configs/metadata.json'\n",
      "2025-03-02 07:07:36,269 - INFO - > input_dict: {'image': 'data/ct_liver_0.nii.gz', 'label_prompt': [1, 26]}\n",
      "2025-03-02 07:07:36,269 - INFO - > output_dtype: 'uint8'\n",
      "2025-03-02 07:07:36,269 - INFO - > separate_folder: False\n",
      "2025-03-02 07:07:36,270 - INFO - > output_ext: '.nii.gz'\n",
      "2025-03-02 07:07:36,270 - INFO - > output_dir: '/tmp/tmps2ucpz9n'\n",
      "2025-03-02 07:07:36,271 - INFO - ---\n",
      "\n",
      "\n",
      "2025-03-02 07:07:36,272 - INFO - Setting logging properties based on config: /root/.cache/torch/hub/bundle/vista3d_v0.5.4/vista3d/configs/logging.conf.\n",
      "2025-03-02 07:07:38,145 - root - INFO - Restored all variables from /root/.cache/torch/hub/bundle/vista3d_v0.5.4/vista3d/models/model.pt\n",
      "2025-03-02 07:07:38,147 - ignite.engine.engine.Vista3dEvaluator - INFO - Engine run resuming from iteration 0, epoch 0 until 1 epochs\n",
      "2025-03-02 07:07:40,170 INFO image_writer.py:197 - writing: /tmp/tmps2ucpz9n/ct_liver_0_trans.nii.gz\n",
      "2025-03-02 07:07:40,489 - ignite.engine.engine.Vista3dEvaluator - INFO - Epoch[1] Complete. Time taken: 00:00:02.341\n",
      "2025-03-02 07:07:40,490 - ignite.engine.engine.Vista3dEvaluator - INFO - Engine run complete. Time taken: 00:00:02.342\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:392: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:397: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "USER: [{'type': 'text', 'text': \"Here is a list of available expert models:\\n<BRATS(args)> Modality: MRI, Task: segmentation, Overview: A pre-trained model for volumetric (3D) segmentation of brain tumor subregions from multimodal MRIs based on BraTS 2018 data, Accuracy: Tumor core (TC): 0.8559 - Whole tumor (WT): 0.9026 - Enhancing tumor (ET): 0.7905 - Average: 0.8518, Valid args are: None\\n<VISTA3D(args)> Modality: CT, Task: segmentation, Overview: domain-specialized interactive foundation model developed for segmenting and annotating human anatomies with precision, Accuracy: 127 organs: 0.792 Dice on average, Valid args are: 'everything', 'hepatic tumor', 'pancreatic tumor', 'lung tumor', 'bone lesion', 'organs', 'cardiovascular', 'gastrointestinal', 'skeleton', or 'muscles'\\n<VISTA2D(args)> Modality: cell imaging, Task: segmentation, Overview: model for cell segmentation, which was trained on a variety of cell imaging outputs, including brightfield, phase-contrast, fluorescence, confocal, or electron microscopy, Accuracy: Good accuracy across several cell imaging datasets, Valid args are: None\\n<CXR(args)> Modality: chest x-ray (CXR), Task: classification, Overview: pre-trained model which are trained on large cohorts of data, Accuracy: Good accuracy across several diverse chest x-rays datasets, Valid args are: None\\nGive the model <NAME(args)> when selecting a suitable expert model.\\n<image>This is a CT image.\\nCan you identify any liver masses or tumors?\"}, {'type': 'image_path', 'image_path': 'data/ct_liver_0_slice57_img.jpg'}]\n",
      "ASSISTANT: [{'type': 'text', 'text': '<VISTA3D(hepatic tumor)>hepatic tumor'}]\n",
      "EXPERT: [{'type': 'text', 'text': 'The results are <image>. The colors in this image describe red: liver, blue: hepatic tumor. '}, {'type': 'image_path', 'image_path': '/tmp/tmpg81swwpt/seg_59c25b45-0563-4d4d-bd0b-cfe776d4cc6e.jpg'}]\n",
      "EXPERT: [{'type': 'text', 'text': 'Use this result to respond to this prompt:\\nCan you identify any liver masses or tumors?'}]\n",
      "ASSISTANT: [{'type': 'text', 'text': 'yes hepatic tumor'}]\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "experiment(\"Can you identify any liver masses or tumors?\", seg_token=\"<image>\", model=\"MONAI/Llama3-VILA-M3-13B\", conv_mode=\"vicuna_v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 23 files: 100%|| 23/23 [00:00<00:00, 101014.65it/s]\n",
      "Loading checkpoint shards: 100%|| 6/6 [00:13<00:00,  2.22s/it]\n",
      "Model 4e68cefd00fa5d9bb309484307cdfc9dec55785f loaded successfully. Context length: 2048\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-03-02 07:08:23,393 - __main__ - INFO - Model 4e68cefd00fa5d9bb309484307cdfc9dec55785f loaded successfully. Context length: 2048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing the prompt: Please provide a detailed description of the image., with max tokens: 1024, temperature: 0.0, top P: 0.9, slice index: 57\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-03-02 07:08:23,395 - __main__ - INFO - Processing the prompt: Please provide a detailed description of the image., with max tokens: 1024, temperature: 0.0, top P: 0.9, slice index: 57\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1295: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
      "  warnings.warn(\n",
      "Expert model ExpertVista3D is being called to process https://developer.download.nvidia.com/assets/Clara/monai/samples/ct_liver_0.nii.gz.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-03-02 07:08:24,426 - __main__ - INFO - Expert model ExpertVista3D is being called to process https://developer.download.nvidia.com/assets/Clara/monai/samples/ct_liver_0.nii.gz.\n",
      "2025-03-02 07:08:24,433 - INFO - --- input summary of monai.bundle.scripts.run ---\n",
      "2025-03-02 07:08:24,436 - INFO - > config_file: '/root/.cache/torch/hub/bundle/vista3d_v0.5.4/vista3d/configs/inference.json'\n",
      "2025-03-02 07:08:24,440 - INFO - > workflow_type: 'infer'\n",
      "2025-03-02 07:08:24,443 - INFO - > bundle_root: '/root/.cache/torch/hub/bundle/vista3d_v0.5.4/vista3d'\n",
      "2025-03-02 07:08:24,449 - INFO - > logging_file: '/root/.cache/torch/hub/bundle/vista3d_v0.5.4/vista3d/configs/logging.conf'\n",
      "2025-03-02 07:08:24,450 - INFO - > meta_file: '/root/.cache/torch/hub/bundle/vista3d_v0.5.4/vista3d/configs/metadata.json'\n",
      "2025-03-02 07:08:24,457 - INFO - > input_dict: {'image': 'data/ct_liver_0.nii.gz'}\n",
      "2025-03-02 07:08:24,457 - INFO - > output_dtype: 'uint8'\n",
      "2025-03-02 07:08:24,458 - INFO - > separate_folder: False\n",
      "2025-03-02 07:08:24,458 - INFO - > output_ext: '.nii.gz'\n",
      "2025-03-02 07:08:24,458 - INFO - > output_dir: '/tmp/tmp_zko2kno'\n",
      "2025-03-02 07:08:24,459 - INFO - ---\n",
      "\n",
      "\n",
      "2025-03-02 07:08:24,460 - INFO - Setting logging properties based on config: /root/.cache/torch/hub/bundle/vista3d_v0.5.4/vista3d/configs/logging.conf.\n",
      "2025-03-02 07:08:26,251 - root - INFO - Restored all variables from /root/.cache/torch/hub/bundle/vista3d_v0.5.4/vista3d/models/model.pt\n",
      "2025-03-02 07:08:26,254 - ignite.engine.engine.Vista3dEvaluator - INFO - Engine run resuming from iteration 0, epoch 0 until 1 epochs\n",
      "2025-03-02 07:08:28,803 INFO image_writer.py:197 - writing: /tmp/tmp_zko2kno/ct_liver_0_trans.nii.gz\n",
      "2025-03-02 07:08:29,162 - ignite.engine.engine.Vista3dEvaluator - INFO - Epoch[1] Complete. Time taken: 00:00:02.908\n",
      "2025-03-02 07:08:29,163 - ignite.engine.engine.Vista3dEvaluator - INFO - Engine run complete. Time taken: 00:00:02.908\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:392: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:397: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "USER: [{'type': 'text', 'text': \"Here is a list of available expert models:\\n<BRATS(args)> Modality: MRI, Task: segmentation, Overview: A pre-trained model for volumetric (3D) segmentation of brain tumor subregions from multimodal MRIs based on BraTS 2018 data, Accuracy: Tumor core (TC): 0.8559 - Whole tumor (WT): 0.9026 - Enhancing tumor (ET): 0.7905 - Average: 0.8518, Valid args are: None\\n<VISTA3D(args)> Modality: CT, Task: segmentation, Overview: domain-specialized interactive foundation model developed for segmenting and annotating human anatomies with precision, Accuracy: 127 organs: 0.792 Dice on average, Valid args are: 'everything', 'hepatic tumor', 'pancreatic tumor', 'lung tumor', 'bone lesion', 'organs', 'cardiovascular', 'gastrointestinal', 'skeleton', or 'muscles'\\n<VISTA2D(args)> Modality: cell imaging, Task: segmentation, Overview: model for cell segmentation, which was trained on a variety of cell imaging outputs, including brightfield, phase-contrast, fluorescence, confocal, or electron microscopy, Accuracy: Good accuracy across several cell imaging datasets, Valid args are: None\\n<CXR(args)> Modality: chest x-ray (CXR), Task: classification, Overview: pre-trained model which are trained on large cohorts of data, Accuracy: Good accuracy across several diverse chest x-rays datasets, Valid args are: None\\nGive the model <NAME(args)> when selecting a suitable expert model.\\n<image>This is a CT image.\\nPlease provide a detailed description of the image.\"}, {'type': 'image_path', 'image_path': 'data/ct_liver_0_slice57_img.jpg'}]\n",
      "ASSISTANT: [{'type': 'text', 'text': 'This looks like a CT image. Let me trigger <VISTA3D(everything)>.'}]\n",
      "EXPERT: [{'type': 'text', 'text': 'The results are <segmentation>. The colors in this image describe red: liver, blue: spleen, yellow: pancreas, magenta: aorta, green: inferior vena cava, indigo: right adrenal gland, darkorange: left adrenal gland, cyan: stomach, pink: left kidney, brown: portal vein and splenic vein, orange: vertebrae L2, lime: vertebrae L1, orange: left rib 9, gold: left rib 10, yellowgreen: left rib 11, darkgreen: left rib 12, red: right rib 10, blue: right rib 11, yellow: right rib 12, magenta: left autochthon, green: right autochthon, indigo: left iliopsoas, darkorange: right iliopsoas, cyan: costal cartilages, pink: prostate, brown: spinal cord. '}, {'type': 'image_path', 'image_path': '/tmp/tmpei0jsuel/seg_c027e0b2-ba76-40c8-9fe9-34649d2c0758.jpg'}]\n",
      "EXPERT: [{'type': 'text', 'text': 'Use this result to respond to this prompt:\\nPlease provide a detailed description of the image.'}]\n",
      "ASSISTANT: [{'type': 'text', 'text': '10 cm desmoplastic trichrome kidney tumor is seen better on the concurrent CT. No pleural effusions. No large pleural effusions. No pneumothorax. The cardiac silhouette is normal in size. No acute osseous abnormalities.'}]\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "experiment(\"Please provide a detailed description of the image.\", seg_token=\"<segmentation>\", model=\"MONAI/Llama3-VILA-M3-13B\", conv_mode=\"vicuna_v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 23 files: 100%|| 23/23 [00:00<00:00, 47756.93it/s]\n",
      "Loading checkpoint shards: 100%|| 6/6 [00:12<00:00,  2.10s/it]\n",
      "Model 4e68cefd00fa5d9bb309484307cdfc9dec55785f loaded successfully. Context length: 2048\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-03-02 07:09:17,606 - __main__ - INFO - Model 4e68cefd00fa5d9bb309484307cdfc9dec55785f loaded successfully. Context length: 2048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing the prompt: Please provide a detailed description of the image., with max tokens: 1024, temperature: 0.0, top P: 0.9, slice index: 57\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-03-02 07:09:17,608 - __main__ - INFO - Processing the prompt: Please provide a detailed description of the image., with max tokens: 1024, temperature: 0.0, top P: 0.9, slice index: 57\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1295: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
      "  warnings.warn(\n",
      "Expert model ExpertVista3D is being called to process https://developer.download.nvidia.com/assets/Clara/monai/samples/ct_liver_0.nii.gz.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-03-02 07:09:18,587 - __main__ - INFO - Expert model ExpertVista3D is being called to process https://developer.download.nvidia.com/assets/Clara/monai/samples/ct_liver_0.nii.gz.\n",
      "2025-03-02 07:09:18,589 - INFO - --- input summary of monai.bundle.scripts.run ---\n",
      "2025-03-02 07:09:18,590 - INFO - > config_file: '/root/.cache/torch/hub/bundle/vista3d_v0.5.4/vista3d/configs/inference.json'\n",
      "2025-03-02 07:09:18,591 - INFO - > workflow_type: 'infer'\n",
      "2025-03-02 07:09:18,591 - INFO - > bundle_root: '/root/.cache/torch/hub/bundle/vista3d_v0.5.4/vista3d'\n",
      "2025-03-02 07:09:18,592 - INFO - > logging_file: '/root/.cache/torch/hub/bundle/vista3d_v0.5.4/vista3d/configs/logging.conf'\n",
      "2025-03-02 07:09:18,593 - INFO - > meta_file: '/root/.cache/torch/hub/bundle/vista3d_v0.5.4/vista3d/configs/metadata.json'\n",
      "2025-03-02 07:09:18,593 - INFO - > input_dict: {'image': 'data/ct_liver_0.nii.gz'}\n",
      "2025-03-02 07:09:18,595 - INFO - > output_dtype: 'uint8'\n",
      "2025-03-02 07:09:18,596 - INFO - > separate_folder: False\n",
      "2025-03-02 07:09:18,596 - INFO - > output_ext: '.nii.gz'\n",
      "2025-03-02 07:09:18,597 - INFO - > output_dir: '/tmp/tmpxf169w5d'\n",
      "2025-03-02 07:09:18,597 - INFO - ---\n",
      "\n",
      "\n",
      "2025-03-02 07:09:18,599 - INFO - Setting logging properties based on config: /root/.cache/torch/hub/bundle/vista3d_v0.5.4/vista3d/configs/logging.conf.\n",
      "2025-03-02 07:09:20,416 - root - INFO - Restored all variables from /root/.cache/torch/hub/bundle/vista3d_v0.5.4/vista3d/models/model.pt\n",
      "2025-03-02 07:09:20,421 - ignite.engine.engine.Vista3dEvaluator - INFO - Engine run resuming from iteration 0, epoch 0 until 1 epochs\n",
      "2025-03-02 07:09:22,991 INFO image_writer.py:197 - writing: /tmp/tmpxf169w5d/ct_liver_0_trans.nii.gz\n",
      "2025-03-02 07:09:23,330 - ignite.engine.engine.Vista3dEvaluator - INFO - Epoch[1] Complete. Time taken: 00:00:02.905\n",
      "2025-03-02 07:09:23,331 - ignite.engine.engine.Vista3dEvaluator - INFO - Engine run complete. Time taken: 00:00:02.906\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:392: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:397: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "USER: [{'type': 'text', 'text': \"Here is a list of available expert models:\\n<BRATS(args)> Modality: MRI, Task: segmentation, Overview: A pre-trained model for volumetric (3D) segmentation of brain tumor subregions from multimodal MRIs based on BraTS 2018 data, Accuracy: Tumor core (TC): 0.8559 - Whole tumor (WT): 0.9026 - Enhancing tumor (ET): 0.7905 - Average: 0.8518, Valid args are: None\\n<VISTA3D(args)> Modality: CT, Task: segmentation, Overview: domain-specialized interactive foundation model developed for segmenting and annotating human anatomies with precision, Accuracy: 127 organs: 0.792 Dice on average, Valid args are: 'everything', 'hepatic tumor', 'pancreatic tumor', 'lung tumor', 'bone lesion', 'organs', 'cardiovascular', 'gastrointestinal', 'skeleton', or 'muscles'\\n<VISTA2D(args)> Modality: cell imaging, Task: segmentation, Overview: model for cell segmentation, which was trained on a variety of cell imaging outputs, including brightfield, phase-contrast, fluorescence, confocal, or electron microscopy, Accuracy: Good accuracy across several cell imaging datasets, Valid args are: None\\n<CXR(args)> Modality: chest x-ray (CXR), Task: classification, Overview: pre-trained model which are trained on large cohorts of data, Accuracy: Good accuracy across several diverse chest x-rays datasets, Valid args are: None\\nGive the model <NAME(args)> when selecting a suitable expert model.\\n<image>This is a CT image.\\nPlease provide a detailed description of the image.\"}, {'type': 'image_path', 'image_path': 'data/ct_liver_0_slice57_img.jpg'}]\n",
      "ASSISTANT: [{'type': 'text', 'text': 'This looks like a CT image. Let me trigger <VISTA3D(everything)>.'}]\n",
      "EXPERT: [{'type': 'text', 'text': 'The results are <image>. The colors in this image describe red: liver, blue: spleen, yellow: pancreas, magenta: aorta, green: inferior vena cava, indigo: right adrenal gland, darkorange: left adrenal gland, cyan: stomach, pink: left kidney, brown: portal vein and splenic vein, orange: vertebrae L2, lime: vertebrae L1, orange: left rib 9, gold: left rib 10, yellowgreen: left rib 11, darkgreen: left rib 12, red: right rib 10, blue: right rib 11, yellow: right rib 12, magenta: left autochthon, green: right autochthon, indigo: left iliopsoas, darkorange: right iliopsoas, cyan: costal cartilages, pink: prostate, brown: spinal cord. '}, {'type': 'image_path', 'image_path': '/tmp/tmp3zkzosd4/seg_af917114-9abb-4b0b-9afc-d111a6d54527.jpg'}]\n",
      "EXPERT: [{'type': 'text', 'text': 'Use this result to respond to this prompt:\\nPlease provide a detailed description of the image.'}]\n",
      "ASSISTANT: [{'type': 'text', 'text': '2.5 cm focal, elliptical opacity within the left lung base, likely within the left lower lobe, is at the upper limits of normal in size. No definite focal consolidation concerning for pneumonia is seen. No large pleural effusions are present. No pneumothorax is present. The cardiac silhouette is at the upper limits of normal in size. The mediastinal contours are normal. No pulmonary edema is present. No acute osseous abnormalities are seen. There is diffuse, mild interstitial edema.'}]\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "experiment(\"Please provide a detailed description of the image.\", seg_token=\"<image>\", model=\"MONAI/Llama3-VILA-M3-13B\", conv_mode=\"vicuna_v1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiment with 3B model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 19 files: 100%|| 19/19 [00:39<00:00,  2.08s/it]\n",
      "Loading checkpoint shards: 100%|| 2/2 [00:06<00:00,  3.02s/it]\n",
      "Model ccbda9298a9b5823dd020f21f24aa0a1c3a70b2d loaded successfully. Context length: 2048\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-03-02 07:11:50,715 - __main__ - INFO - Model ccbda9298a9b5823dd020f21f24aa0a1c3a70b2d loaded successfully. Context length: 2048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing the prompt: Can you identify any liver masses or tumors?, with max tokens: 1024, temperature: 0.0, top P: 0.9, slice index: 57\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-03-02 07:11:50,717 - __main__ - INFO - Processing the prompt: Can you identify any liver masses or tumors?, with max tokens: 1024, temperature: 0.0, top P: 0.9, slice index: 57\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Expert model ExpertVista3D is being called to process https://developer.download.nvidia.com/assets/Clara/monai/samples/ct_liver_0.nii.gz.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-03-02 07:11:51,354 - __main__ - INFO - Expert model ExpertVista3D is being called to process https://developer.download.nvidia.com/assets/Clara/monai/samples/ct_liver_0.nii.gz.\n",
      "2025-03-02 07:11:51,355 - INFO - --- input summary of monai.bundle.scripts.run ---\n",
      "2025-03-02 07:11:51,356 - INFO - > config_file: '/root/.cache/torch/hub/bundle/vista3d_v0.5.4/vista3d/configs/inference.json'\n",
      "2025-03-02 07:11:51,356 - INFO - > workflow_type: 'infer'\n",
      "2025-03-02 07:11:51,356 - INFO - > bundle_root: '/root/.cache/torch/hub/bundle/vista3d_v0.5.4/vista3d'\n",
      "2025-03-02 07:11:51,357 - INFO - > logging_file: '/root/.cache/torch/hub/bundle/vista3d_v0.5.4/vista3d/configs/logging.conf'\n",
      "2025-03-02 07:11:51,357 - INFO - > meta_file: '/root/.cache/torch/hub/bundle/vista3d_v0.5.4/vista3d/configs/metadata.json'\n",
      "2025-03-02 07:11:51,358 - INFO - > input_dict: {'image': 'data/ct_liver_0.nii.gz', 'label_prompt': [1, 26]}\n",
      "2025-03-02 07:11:51,358 - INFO - > output_dtype: 'uint8'\n",
      "2025-03-02 07:11:51,359 - INFO - > separate_folder: False\n",
      "2025-03-02 07:11:51,359 - INFO - > output_ext: '.nii.gz'\n",
      "2025-03-02 07:11:51,359 - INFO - > output_dir: '/tmp/tmpmjqq24av'\n",
      "2025-03-02 07:11:51,360 - INFO - ---\n",
      "\n",
      "\n",
      "2025-03-02 07:11:51,360 - INFO - Setting logging properties based on config: /root/.cache/torch/hub/bundle/vista3d_v0.5.4/vista3d/configs/logging.conf.\n",
      "2025-03-02 07:11:53,166 - root - INFO - Restored all variables from /root/.cache/torch/hub/bundle/vista3d_v0.5.4/vista3d/models/model.pt\n",
      "2025-03-02 07:11:53,167 - ignite.engine.engine.Vista3dEvaluator - INFO - Engine run resuming from iteration 0, epoch 0 until 1 epochs\n",
      "2025-03-02 07:11:57,217 INFO image_writer.py:197 - writing: /tmp/tmpmjqq24av/ct_liver_0_trans.nii.gz\n",
      "2025-03-02 07:11:57,483 - ignite.engine.engine.Vista3dEvaluator - INFO - Epoch[1] Complete. Time taken: 00:00:04.315\n",
      "2025-03-02 07:11:57,485 - ignite.engine.engine.Vista3dEvaluator - INFO - Engine run complete. Time taken: 00:00:04.317\n",
      "====================================================================================================\n",
      "USER: [{'type': 'text', 'text': \"Here is a list of available expert models:\\n<BRATS(args)> Modality: MRI, Task: segmentation, Overview: A pre-trained model for volumetric (3D) segmentation of brain tumor subregions from multimodal MRIs based on BraTS 2018 data, Accuracy: Tumor core (TC): 0.8559 - Whole tumor (WT): 0.9026 - Enhancing tumor (ET): 0.7905 - Average: 0.8518, Valid args are: None\\n<VISTA3D(args)> Modality: CT, Task: segmentation, Overview: domain-specialized interactive foundation model developed for segmenting and annotating human anatomies with precision, Accuracy: 127 organs: 0.792 Dice on average, Valid args are: 'everything', 'hepatic tumor', 'pancreatic tumor', 'lung tumor', 'bone lesion', 'organs', 'cardiovascular', 'gastrointestinal', 'skeleton', or 'muscles'\\n<VISTA2D(args)> Modality: cell imaging, Task: segmentation, Overview: model for cell segmentation, which was trained on a variety of cell imaging outputs, including brightfield, phase-contrast, fluorescence, confocal, or electron microscopy, Accuracy: Good accuracy across several cell imaging datasets, Valid args are: None\\n<CXR(args)> Modality: chest x-ray (CXR), Task: classification, Overview: pre-trained model which are trained on large cohorts of data, Accuracy: Good accuracy across several diverse chest x-rays datasets, Valid args are: None\\nGive the model <NAME(args)> when selecting a suitable expert model.\\n<image>This is a CT image.\\nCan you identify any liver masses or tumors?\"}, {'type': 'image_path', 'image_path': 'data/ct_liver_0_slice57_img.jpg'}]\n",
      "ASSISTANT: [{'type': 'text', 'text': 'Let me trigger <VISTA3D(hepatic tumor)>.'}]\n",
      "EXPERT: [{'type': 'text', 'text': 'The results are <segmentation>. The colors in this image describe red: liver, blue: hepatic tumor. '}, {'type': 'image_path', 'image_path': '/tmp/tmp5q8syfv0/seg_df233a69-0c0a-4792-aa6d-a0625fc3cf7d.jpg'}]\n",
      "EXPERT: [{'type': 'text', 'text': 'Use this result to respond to this prompt:\\nCan you identify any liver masses or tumors?'}]\n",
      "ASSISTANT: [{'type': 'text', 'text': 'yes'}]\n",
      "====================================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:392: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:397: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "experiment(\"Can you identify any liver masses or tumors?\", seg_token=\"<segmentation>\", model=\"MONAI/Llama3-VILA-M3-3B\", conv_mode=\"vicuna_v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 19 files: 100%|| 19/19 [00:00<00:00, 110071.51it/s]\n",
      "Loading checkpoint shards: 100%|| 2/2 [00:06<00:00,  3.13s/it]\n",
      "Model ccbda9298a9b5823dd020f21f24aa0a1c3a70b2d loaded successfully. Context length: 2048\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-03-02 07:12:25,339 - __main__ - INFO - Model ccbda9298a9b5823dd020f21f24aa0a1c3a70b2d loaded successfully. Context length: 2048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing the prompt: Can you identify any liver masses or tumors?, with max tokens: 1024, temperature: 0.0, top P: 0.9, slice index: 57\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-03-02 07:12:25,341 - __main__ - INFO - Processing the prompt: Can you identify any liver masses or tumors?, with max tokens: 1024, temperature: 0.0, top P: 0.9, slice index: 57\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Expert model ExpertVista3D is being called to process https://developer.download.nvidia.com/assets/Clara/monai/samples/ct_liver_0.nii.gz.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-03-02 07:12:25,966 - __main__ - INFO - Expert model ExpertVista3D is being called to process https://developer.download.nvidia.com/assets/Clara/monai/samples/ct_liver_0.nii.gz.\n",
      "2025-03-02 07:12:25,968 - INFO - --- input summary of monai.bundle.scripts.run ---\n",
      "2025-03-02 07:12:25,968 - INFO - > config_file: '/root/.cache/torch/hub/bundle/vista3d_v0.5.4/vista3d/configs/inference.json'\n",
      "2025-03-02 07:12:25,969 - INFO - > workflow_type: 'infer'\n",
      "2025-03-02 07:12:25,969 - INFO - > bundle_root: '/root/.cache/torch/hub/bundle/vista3d_v0.5.4/vista3d'\n",
      "2025-03-02 07:12:25,969 - INFO - > logging_file: '/root/.cache/torch/hub/bundle/vista3d_v0.5.4/vista3d/configs/logging.conf'\n",
      "2025-03-02 07:12:25,970 - INFO - > meta_file: '/root/.cache/torch/hub/bundle/vista3d_v0.5.4/vista3d/configs/metadata.json'\n",
      "2025-03-02 07:12:25,970 - INFO - > input_dict: {'image': 'data/ct_liver_0.nii.gz', 'label_prompt': [1, 26]}\n",
      "2025-03-02 07:12:25,971 - INFO - > output_dtype: 'uint8'\n",
      "2025-03-02 07:12:25,971 - INFO - > separate_folder: False\n",
      "2025-03-02 07:12:25,972 - INFO - > output_ext: '.nii.gz'\n",
      "2025-03-02 07:12:25,972 - INFO - > output_dir: '/tmp/tmpe9c_ommk'\n",
      "2025-03-02 07:12:25,972 - INFO - ---\n",
      "\n",
      "\n",
      "2025-03-02 07:12:25,973 - INFO - Setting logging properties based on config: /root/.cache/torch/hub/bundle/vista3d_v0.5.4/vista3d/configs/logging.conf.\n",
      "2025-03-02 07:12:27,882 - root - INFO - Restored all variables from /root/.cache/torch/hub/bundle/vista3d_v0.5.4/vista3d/models/model.pt\n",
      "2025-03-02 07:12:27,884 - ignite.engine.engine.Vista3dEvaluator - INFO - Engine run resuming from iteration 0, epoch 0 until 1 epochs\n",
      "2025-03-02 07:12:30,946 INFO image_writer.py:197 - writing: /tmp/tmpe9c_ommk/ct_liver_0_trans.nii.gz\n",
      "2025-03-02 07:12:31,481 - ignite.engine.engine.Vista3dEvaluator - INFO - Epoch[1] Complete. Time taken: 00:00:03.597\n",
      "2025-03-02 07:12:31,483 - ignite.engine.engine.Vista3dEvaluator - INFO - Engine run complete. Time taken: 00:00:03.598\n",
      "====================================================================================================\n",
      "USER: [{'type': 'text', 'text': \"Here is a list of available expert models:\\n<BRATS(args)> Modality: MRI, Task: segmentation, Overview: A pre-trained model for volumetric (3D) segmentation of brain tumor subregions from multimodal MRIs based on BraTS 2018 data, Accuracy: Tumor core (TC): 0.8559 - Whole tumor (WT): 0.9026 - Enhancing tumor (ET): 0.7905 - Average: 0.8518, Valid args are: None\\n<VISTA3D(args)> Modality: CT, Task: segmentation, Overview: domain-specialized interactive foundation model developed for segmenting and annotating human anatomies with precision, Accuracy: 127 organs: 0.792 Dice on average, Valid args are: 'everything', 'hepatic tumor', 'pancreatic tumor', 'lung tumor', 'bone lesion', 'organs', 'cardiovascular', 'gastrointestinal', 'skeleton', or 'muscles'\\n<VISTA2D(args)> Modality: cell imaging, Task: segmentation, Overview: model for cell segmentation, which was trained on a variety of cell imaging outputs, including brightfield, phase-contrast, fluorescence, confocal, or electron microscopy, Accuracy: Good accuracy across several cell imaging datasets, Valid args are: None\\n<CXR(args)> Modality: chest x-ray (CXR), Task: classification, Overview: pre-trained model which are trained on large cohorts of data, Accuracy: Good accuracy across several diverse chest x-rays datasets, Valid args are: None\\nGive the model <NAME(args)> when selecting a suitable expert model.\\n<image>This is a CT image.\\nCan you identify any liver masses or tumors?\"}, {'type': 'image_path', 'image_path': 'data/ct_liver_0_slice57_img.jpg'}]\n",
      "ASSISTANT: [{'type': 'text', 'text': 'Let me trigger <VISTA3D(hepatic tumor)>.'}]\n",
      "EXPERT: [{'type': 'text', 'text': 'The results are <image>. The colors in this image describe red: liver, blue: hepatic tumor. '}, {'type': 'image_path', 'image_path': '/tmp/tmpk15nekmv/seg_b80eeba6-4391-43bc-9eb7-0870aadecb4a.jpg'}]\n",
      "EXPERT: [{'type': 'text', 'text': 'Use this result to respond to this prompt:\\nCan you identify any liver masses or tumors?'}]\n",
      "ASSISTANT: [{'type': 'text', 'text': 'yes'}]\n",
      "====================================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:392: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:397: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "experiment(\"Can you identify any liver masses or tumors?\", seg_token=\"<image>\", model=\"MONAI/Llama3-VILA-M3-3B\", conv_mode=\"vicuna_v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 19 files: 100%|| 19/19 [00:00<00:00, 66354.52it/s]\n",
      "Loading checkpoint shards: 100%|| 2/2 [00:06<00:00,  3.05s/it]\n",
      "Model ccbda9298a9b5823dd020f21f24aa0a1c3a70b2d loaded successfully. Context length: 2048\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-03-02 07:13:05,886 - __main__ - INFO - Model ccbda9298a9b5823dd020f21f24aa0a1c3a70b2d loaded successfully. Context length: 2048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing the prompt: Please provide a detailed description of the image., with max tokens: 1024, temperature: 0.0, top P: 0.9, slice index: 57\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-03-02 07:13:05,888 - __main__ - INFO - Processing the prompt: Please provide a detailed description of the image., with max tokens: 1024, temperature: 0.0, top P: 0.9, slice index: 57\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Expert model ExpertVista3D is being called to process https://developer.download.nvidia.com/assets/Clara/monai/samples/ct_liver_0.nii.gz.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-03-02 07:13:06,627 - __main__ - INFO - Expert model ExpertVista3D is being called to process https://developer.download.nvidia.com/assets/Clara/monai/samples/ct_liver_0.nii.gz.\n",
      "2025-03-02 07:13:06,630 - INFO - --- input summary of monai.bundle.scripts.run ---\n",
      "2025-03-02 07:13:06,631 - INFO - > config_file: '/root/.cache/torch/hub/bundle/vista3d_v0.5.4/vista3d/configs/inference.json'\n",
      "2025-03-02 07:13:06,632 - INFO - > workflow_type: 'infer'\n",
      "2025-03-02 07:13:06,633 - INFO - > bundle_root: '/root/.cache/torch/hub/bundle/vista3d_v0.5.4/vista3d'\n",
      "2025-03-02 07:13:06,633 - INFO - > logging_file: '/root/.cache/torch/hub/bundle/vista3d_v0.5.4/vista3d/configs/logging.conf'\n",
      "2025-03-02 07:13:06,634 - INFO - > meta_file: '/root/.cache/torch/hub/bundle/vista3d_v0.5.4/vista3d/configs/metadata.json'\n",
      "2025-03-02 07:13:06,634 - INFO - > input_dict: {'image': 'data/ct_liver_0.nii.gz'}\n",
      "2025-03-02 07:13:06,635 - INFO - > output_dtype: 'uint8'\n",
      "2025-03-02 07:13:06,636 - INFO - > separate_folder: False\n",
      "2025-03-02 07:13:06,636 - INFO - > output_ext: '.nii.gz'\n",
      "2025-03-02 07:13:06,637 - INFO - > output_dir: '/tmp/tmpjbgii8it'\n",
      "2025-03-02 07:13:06,638 - INFO - ---\n",
      "\n",
      "\n",
      "2025-03-02 07:13:06,638 - INFO - Setting logging properties based on config: /root/.cache/torch/hub/bundle/vista3d_v0.5.4/vista3d/configs/logging.conf.\n",
      "2025-03-02 07:13:08,484 - root - INFO - Restored all variables from /root/.cache/torch/hub/bundle/vista3d_v0.5.4/vista3d/models/model.pt\n",
      "2025-03-02 07:13:08,486 - ignite.engine.engine.Vista3dEvaluator - INFO - Engine run resuming from iteration 0, epoch 0 until 1 epochs\n",
      "2025-03-02 07:13:11,070 INFO image_writer.py:197 - writing: /tmp/tmpjbgii8it/ct_liver_0_trans.nii.gz\n",
      "2025-03-02 07:13:11,422 - ignite.engine.engine.Vista3dEvaluator - INFO - Epoch[1] Complete. Time taken: 00:00:02.936\n",
      "2025-03-02 07:13:11,423 - ignite.engine.engine.Vista3dEvaluator - INFO - Engine run complete. Time taken: 00:00:02.937\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:392: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:397: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "USER: [{'type': 'text', 'text': \"Here is a list of available expert models:\\n<BRATS(args)> Modality: MRI, Task: segmentation, Overview: A pre-trained model for volumetric (3D) segmentation of brain tumor subregions from multimodal MRIs based on BraTS 2018 data, Accuracy: Tumor core (TC): 0.8559 - Whole tumor (WT): 0.9026 - Enhancing tumor (ET): 0.7905 - Average: 0.8518, Valid args are: None\\n<VISTA3D(args)> Modality: CT, Task: segmentation, Overview: domain-specialized interactive foundation model developed for segmenting and annotating human anatomies with precision, Accuracy: 127 organs: 0.792 Dice on average, Valid args are: 'everything', 'hepatic tumor', 'pancreatic tumor', 'lung tumor', 'bone lesion', 'organs', 'cardiovascular', 'gastrointestinal', 'skeleton', or 'muscles'\\n<VISTA2D(args)> Modality: cell imaging, Task: segmentation, Overview: model for cell segmentation, which was trained on a variety of cell imaging outputs, including brightfield, phase-contrast, fluorescence, confocal, or electron microscopy, Accuracy: Good accuracy across several cell imaging datasets, Valid args are: None\\n<CXR(args)> Modality: chest x-ray (CXR), Task: classification, Overview: pre-trained model which are trained on large cohorts of data, Accuracy: Good accuracy across several diverse chest x-rays datasets, Valid args are: None\\nGive the model <NAME(args)> when selecting a suitable expert model.\\n<image>This is a CT image.\\nPlease provide a detailed description of the image.\"}, {'type': 'image_path', 'image_path': 'data/ct_liver_0_slice57_img.jpg'}]\n",
      "ASSISTANT: [{'type': 'text', 'text': 'This looks like a CT image. Let me trigger <VISTA3D(everything)>.'}]\n",
      "EXPERT: [{'type': 'text', 'text': 'The results are <segmentation>. The colors in this image describe red: liver, blue: spleen, yellow: pancreas, magenta: aorta, green: inferior vena cava, indigo: right adrenal gland, darkorange: left adrenal gland, cyan: stomach, pink: left kidney, brown: portal vein and splenic vein, orange: vertebrae L2, lime: vertebrae L1, orange: left rib 9, gold: left rib 10, yellowgreen: left rib 11, darkgreen: left rib 12, red: right rib 10, blue: right rib 11, yellow: right rib 12, magenta: left autochthon, green: right autochthon, indigo: left iliopsoas, darkorange: right iliopsoas, cyan: costal cartilages, pink: prostate, brown: spinal cord. '}, {'type': 'image_path', 'image_path': '/tmp/tmpjjai8w9h/seg_68ecb857-ef3d-4d64-87ba-1da658be33f3.jpg'}]\n",
      "EXPERT: [{'type': 'text', 'text': 'Use this result to respond to this prompt:\\nPlease provide a detailed description of the image.'}]\n",
      "ASSISTANT: [{'type': 'text', 'text': ''}]\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "experiment(\"Please provide a detailed description of the image.\", seg_token=\"<segmentation>\", model=\"MONAI/Llama3-VILA-M3-3B\", conv_mode=\"vicuna_v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 19 files: 100%|| 19/19 [00:00<00:00, 94758.35it/s]\n",
      "Loading checkpoint shards: 100%|| 2/2 [00:06<00:00,  3.06s/it]\n",
      "Model ccbda9298a9b5823dd020f21f24aa0a1c3a70b2d loaded successfully. Context length: 2048\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-03-02 07:13:41,605 - __main__ - INFO - Model ccbda9298a9b5823dd020f21f24aa0a1c3a70b2d loaded successfully. Context length: 2048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing the prompt: Please provide a detailed description of the image., with max tokens: 1024, temperature: 0.0, top P: 0.9, slice index: 57\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-03-02 07:13:41,607 - __main__ - INFO - Processing the prompt: Please provide a detailed description of the image., with max tokens: 1024, temperature: 0.0, top P: 0.9, slice index: 57\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Expert model ExpertVista3D is being called to process https://developer.download.nvidia.com/assets/Clara/monai/samples/ct_liver_0.nii.gz.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-03-02 07:13:42,374 - __main__ - INFO - Expert model ExpertVista3D is being called to process https://developer.download.nvidia.com/assets/Clara/monai/samples/ct_liver_0.nii.gz.\n",
      "2025-03-02 07:13:42,379 - INFO - --- input summary of monai.bundle.scripts.run ---\n",
      "2025-03-02 07:13:42,379 - INFO - > config_file: '/root/.cache/torch/hub/bundle/vista3d_v0.5.4/vista3d/configs/inference.json'\n",
      "2025-03-02 07:13:42,380 - INFO - > workflow_type: 'infer'\n",
      "2025-03-02 07:13:42,380 - INFO - > bundle_root: '/root/.cache/torch/hub/bundle/vista3d_v0.5.4/vista3d'\n",
      "2025-03-02 07:13:42,381 - INFO - > logging_file: '/root/.cache/torch/hub/bundle/vista3d_v0.5.4/vista3d/configs/logging.conf'\n",
      "2025-03-02 07:13:42,382 - INFO - > meta_file: '/root/.cache/torch/hub/bundle/vista3d_v0.5.4/vista3d/configs/metadata.json'\n",
      "2025-03-02 07:13:42,382 - INFO - > input_dict: {'image': 'data/ct_liver_0.nii.gz'}\n",
      "2025-03-02 07:13:42,382 - INFO - > output_dtype: 'uint8'\n",
      "2025-03-02 07:13:42,383 - INFO - > separate_folder: False\n",
      "2025-03-02 07:13:42,383 - INFO - > output_ext: '.nii.gz'\n",
      "2025-03-02 07:13:42,384 - INFO - > output_dir: '/tmp/tmp016s3cht'\n",
      "2025-03-02 07:13:42,384 - INFO - ---\n",
      "\n",
      "\n",
      "2025-03-02 07:13:42,385 - INFO - Setting logging properties based on config: /root/.cache/torch/hub/bundle/vista3d_v0.5.4/vista3d/configs/logging.conf.\n",
      "2025-03-02 07:13:44,383 - root - INFO - Restored all variables from /root/.cache/torch/hub/bundle/vista3d_v0.5.4/vista3d/models/model.pt\n",
      "2025-03-02 07:13:44,384 - ignite.engine.engine.Vista3dEvaluator - INFO - Engine run resuming from iteration 0, epoch 0 until 1 epochs\n",
      "2025-03-02 07:13:47,181 INFO image_writer.py:197 - writing: /tmp/tmp016s3cht/ct_liver_0_trans.nii.gz\n",
      "2025-03-02 07:13:47,461 - ignite.engine.engine.Vista3dEvaluator - INFO - Epoch[1] Complete. Time taken: 00:00:03.072\n",
      "2025-03-02 07:13:47,462 - ignite.engine.engine.Vista3dEvaluator - INFO - Engine run complete. Time taken: 00:00:03.074\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:392: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:397: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "USER: [{'type': 'text', 'text': \"Here is a list of available expert models:\\n<BRATS(args)> Modality: MRI, Task: segmentation, Overview: A pre-trained model for volumetric (3D) segmentation of brain tumor subregions from multimodal MRIs based on BraTS 2018 data, Accuracy: Tumor core (TC): 0.8559 - Whole tumor (WT): 0.9026 - Enhancing tumor (ET): 0.7905 - Average: 0.8518, Valid args are: None\\n<VISTA3D(args)> Modality: CT, Task: segmentation, Overview: domain-specialized interactive foundation model developed for segmenting and annotating human anatomies with precision, Accuracy: 127 organs: 0.792 Dice on average, Valid args are: 'everything', 'hepatic tumor', 'pancreatic tumor', 'lung tumor', 'bone lesion', 'organs', 'cardiovascular', 'gastrointestinal', 'skeleton', or 'muscles'\\n<VISTA2D(args)> Modality: cell imaging, Task: segmentation, Overview: model for cell segmentation, which was trained on a variety of cell imaging outputs, including brightfield, phase-contrast, fluorescence, confocal, or electron microscopy, Accuracy: Good accuracy across several cell imaging datasets, Valid args are: None\\n<CXR(args)> Modality: chest x-ray (CXR), Task: classification, Overview: pre-trained model which are trained on large cohorts of data, Accuracy: Good accuracy across several diverse chest x-rays datasets, Valid args are: None\\nGive the model <NAME(args)> when selecting a suitable expert model.\\n<image>This is a CT image.\\nPlease provide a detailed description of the image.\"}, {'type': 'image_path', 'image_path': 'data/ct_liver_0_slice57_img.jpg'}]\n",
      "ASSISTANT: [{'type': 'text', 'text': 'This looks like a CT image. Let me trigger <VISTA3D(everything)>.'}]\n",
      "EXPERT: [{'type': 'text', 'text': 'The results are <image>. The colors in this image describe red: liver, blue: spleen, yellow: pancreas, magenta: aorta, green: inferior vena cava, indigo: right adrenal gland, darkorange: left adrenal gland, cyan: stomach, pink: left kidney, brown: portal vein and splenic vein, orange: vertebrae L2, lime: vertebrae L1, orange: left rib 9, gold: left rib 10, yellowgreen: left rib 11, darkgreen: left rib 12, red: right rib 10, blue: right rib 11, yellow: right rib 12, magenta: left autochthon, green: right autochthon, indigo: left iliopsoas, darkorange: right iliopsoas, cyan: costal cartilages, pink: prostate, brown: spinal cord. '}, {'type': 'image_path', 'image_path': '/tmp/tmp89reug8o/seg_44f2720f-b156-4afd-a0c1-8a79167a5456.jpg'}]\n",
      "EXPERT: [{'type': 'text', 'text': 'Use this result to respond to this prompt:\\nPlease provide a detailed description of the image.'}]\n",
      "ASSISTANT: [{'type': 'text', 'text': '2. The cardiac silhouette is at the upper limits of normal in size. The lungs are clear. The pulmonary vascularity is normal. No pleural effusions. No large pleural effusions. No pulmonary edema. No pneumothorax. No focal consolidation is seen. No definite focal consolidation concerning for pneumonia. The mediastinal contours are normal. The hilar contours are normal. The mediastinal contours are stable. There is mild pulmonary vascular congestion. There is mild interstitial edema. The cardiac silhouette is normal in size. The cardiac silhouette is mildly enlarged. The cardiac silhouette is enlarged. The cardiac silhouette is stable. The lungs are low in volume. The lungs are hyperinflated. The mediastinal contours are unchanged. The hilar contours are unchanged. The lungs are at the upper limits of normal in size. The lungs are normal in size. The lungs are stable. The lungs are normal. The lungs are normal in size. The lungs are at the upper limits of normal in size. The lungs are hyperinflated. The lungs are clear. The lungs are at the upper limits of normal in size. The lungs are normal in size. The lungs are stable. The lungs are normal. The lungs are stable. The cardiac silhouette is normal in size. The cardiac silhouette is stable. The cardiac silhouette is at the upper limits of normal in size. The cardiac silhouette is enlarged. The cardiac silhouette is enlarged. The cardiac silhouette is stable. There is mild pulmonary vascular congestion. There is mild interstitial edema.'}]\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "experiment(\"Please provide a detailed description of the image.\", seg_token=\"<image>\", model=\"MONAI/Llama3-VILA-M3-3B\", conv_mode=\"vicuna_v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
